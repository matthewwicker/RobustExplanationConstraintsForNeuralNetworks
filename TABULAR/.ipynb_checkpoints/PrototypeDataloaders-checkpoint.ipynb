{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3acfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "class TabularDataset:\n",
    "\n",
    "    def __init__(self, X_raw, y_raw, sensitive_features=[], drop_columns=[], drop_first=False, drop_first_labels=True):\n",
    "        \"\"\"\n",
    "        X_raw: features dataframe\n",
    "        y_raw: labels dataframe or column label\n",
    "        sensitive_features: the features considered sensitive\n",
    "        drop_columns: the columns considered superfluous and to be deleted\n",
    "        drop_first: whether to drop first when one-hot encoding features\n",
    "        drop_first_labels: whether to drop first when one-hot encoding labels\n",
    "        \"\"\"\n",
    "\n",
    "        self.sensitive_features = sensitive_features\n",
    "\n",
    "        X_raw.drop(columns=drop_columns, inplace=True)\n",
    "        self.X_raw = X_raw\n",
    "\n",
    "        num_cols, cat_cols, sens_num_cols, sens_cat_cols = self.get_num_cat_columns_sorted(X_raw, sensitive_features)\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.sens_num_cols = sens_num_cols\n",
    "        self.sens_cat_cols = sens_cat_cols\n",
    "        self.original_columns = X_raw.columns.values.tolist()\n",
    "\n",
    "        X_df_all, y_df = self.prepare_dataset(X_raw, y_raw, drop_first=drop_first, drop_first_labels=drop_first_labels, drop_columns=drop_columns)\n",
    "\n",
    "        self.y_df = y_df\n",
    "\n",
    "        # Map from original column names to all new encoded ones\n",
    "        all_columns_map = {}\n",
    "        encoded_columns = X_df_all.columns.values.tolist()\n",
    "        for c in self.original_columns:\n",
    "            all_columns_map[c] = [ encoded_columns.index(e_c) for e_c in encoded_columns if e_c == c or e_c.startswith(c + '_') ]\n",
    "\n",
    "        # List of list of the indexes of each sensitive features\n",
    "        encoded_features = X_df_all.columns.values.tolist()\n",
    "        sensitive_idxs = []\n",
    "        sensitive_idxs_flat = []\n",
    "        for sf in sensitive_features:\n",
    "            sensitive_idxs.append(all_columns_map[sf])\n",
    "            sensitive_idxs_flat.extend(all_columns_map[sf])\n",
    "        all_idxs = [i for i in range(len(X_df_all.columns))]\n",
    "        valid_idxs = [i for i in all_idxs if i not in sensitive_idxs_flat]\n",
    "\n",
    "        # Datasets with one-hot encoded columns of each sensitive feature\n",
    "        self.sensitive_dfs = [X_df_all.iloc[:, idxs] for idxs in sensitive_idxs]\n",
    "\n",
    "        # Dataset with all features but the sensitive ones\n",
    "        self.X_df = X_df_all.iloc[:, valid_idxs]\n",
    "\n",
    "        self.columns_map = {}\n",
    "        encoded_columns = self.X_df.columns.values.tolist()\n",
    "        for c in num_cols + cat_cols:\n",
    "            self.columns_map[c] = [ encoded_columns.index(e_c) for e_c in encoded_columns if e_c == c or e_c.startswith(c + '_') ]\n",
    "\n",
    "    def get_num_cat_columns_sorted(self, X_df, sensitive_features):\n",
    "        num_cols = []\n",
    "        cat_cols = []\n",
    "\n",
    "        sens_num_cols = []\n",
    "        sens_cat_cols = []\n",
    "\n",
    "        for c in X_df.columns:\n",
    "            if c in sensitive_features:\n",
    "                if X_df[c].dtype == 'object' or X_df[c].dtype.name == 'category':\n",
    "                    sens_cat_cols.append(c)\n",
    "                else:\n",
    "                    sens_num_cols.append(c)\n",
    "            else:\n",
    "                if X_df[c].dtype == 'object' or X_df[c].dtype.name == 'category':\n",
    "                    cat_cols.append(c)\n",
    "                else:\n",
    "                    num_cols.append(c)\n",
    "\n",
    "        num_cols.sort()\n",
    "        cat_cols.sort()\n",
    "        sens_num_cols.sort()\n",
    "        sens_cat_cols.sort()\n",
    "\n",
    "        return num_cols, cat_cols, sens_num_cols, sens_cat_cols\n",
    "\n",
    "    def scale_num_cols(self, X_df, num_cols):\n",
    "        \"\"\"\n",
    "        X_df: features dataframe\n",
    "        num_cols: name of all numerical columns to be scaled\n",
    "        returns: feature dataframe with scaled numerical features\n",
    "        \"\"\"\n",
    "        X_df_scaled = X_df.copy()\n",
    "        scaler = MinMaxScaler()\n",
    "        X_num = scaler.fit_transform(X_df_scaled[num_cols])\n",
    "\n",
    "        for i, c in enumerate(num_cols):\n",
    "            X_df_scaled[c] = X_num[:,i]\n",
    "\n",
    "        return X_df_scaled\n",
    "\n",
    "    def process_num_cat_columns(self, X_df, drop_first):\n",
    "        \"\"\"\n",
    "        X_df: features dataframe\n",
    "        returns: feature dataframe with scaled numerical features and one-hot encoded categorical features\n",
    "        \"\"\"\n",
    "        num_cols = []\n",
    "        cat_cols = []\n",
    "\n",
    "        for c in X_df.columns:\n",
    "            if X_df[c].dtype == 'object' or X_df[c].dtype.name == 'category':\n",
    "                cat_cols.append(c)\n",
    "            else:\n",
    "                num_cols.append(c)\n",
    "\n",
    "        # TODO: need to think about this drop_first\n",
    "        X_df_encoded = pd.get_dummies(X_df, columns=cat_cols, drop_first=drop_first)\n",
    "\n",
    "        cat_cols = list(set(X_df_encoded.columns) - set(num_cols))\n",
    "\n",
    "        num_cols.sort()\n",
    "        cat_cols.sort()\n",
    "\n",
    "        X_df_encoded_scaled = self.scale_num_cols(X_df_encoded, num_cols)\n",
    "\n",
    "        return X_df_encoded_scaled[num_cols + cat_cols]\n",
    "\n",
    "\n",
    "    def process_labels(self, X_df, y_df, drop_first):\n",
    "        X_processed = X_df.copy()\n",
    "        if isinstance(y_df, str):\n",
    "            prefix = y_df\n",
    "            y_columns = [ c for c in X_processed.columns if c == prefix or c.startswith(prefix + '_') ]\n",
    "            y_processed = X_df[y_columns]\n",
    "            X_processed.drop(columns=y_columns, inplace=True)\n",
    "        else:\n",
    "            y_processed = pd.get_dummies(y_df, drop_first=drop_first)\n",
    "\n",
    "        return X_processed, y_processed\n",
    "\n",
    "\n",
    "    def prepare_dataset(self, X_df_original, y_df_original, drop_first, drop_first_labels, drop_columns=[]):\n",
    "        \"\"\"\n",
    "        X_df_original: features dataframe\n",
    "        y_df_original: labels dataframe\n",
    "        returns:\n",
    "            - feature dataframe with scaled numerical features and one-hot encoded categorical features\n",
    "            - one hot encoded labels, with drop_first option\n",
    "        \"\"\"\n",
    "        X_df = X_df_original.copy()\n",
    "\n",
    "        X_processed = self.process_num_cat_columns(X_df, drop_first)\n",
    "\n",
    "        X_processed, y_processed = self.process_labels(X_processed, y_df_original, drop_first_labels)\n",
    "\n",
    "        return X_processed, y_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87075213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "DATA_ADULT_TRAIN = './Datasets/adult.data.csv'\n",
    "DATA_ADULT_TEST = './Datasets/adult.test.csv'\n",
    "DATA_CRIME_FILENAME = './Datasets/crime.csv'\n",
    "DATA_GERMAN_FILENAME = './Datasets/german.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1b1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age: continuous.\n",
    "# workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov,\n",
    "#     Without-pay, Never-worked.\n",
    "# fnlwgt: continuous.\n",
    "# education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th,\n",
    "#     7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "# education-num: continuous.\n",
    "# marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed,\n",
    "#     Married-spouse-absent, Married-AF-spouse.\n",
    "# occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty,\n",
    "#     Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving,\n",
    "#     Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "# relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "# race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "# sex: Female, Male.\n",
    "# capital-gain: continuous.\n",
    "# capital-loss: continuous.\n",
    "# hours-per-week: continuous.\n",
    "# native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany,\n",
    "#     Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras,\n",
    "#     Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France,\n",
    "#     Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua,\n",
    "#     Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "def get_adult_data(sensitive_features, drop_columns=[], test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    train_path: path to training data\n",
    "    test_path: path to test data\n",
    "    returns: tuple of training features, training labels, test features and test labels\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(DATA_ADULT_TRAIN, na_values='?').dropna()\n",
    "    test_df = pd.read_csv(DATA_ADULT_TEST, na_values='?').dropna()\n",
    "    target = 'target'\n",
    "\n",
    "    X_train = train_df.drop(columns=[target])\n",
    "    y_train = train_df[[target]]\n",
    "    X_test = test_df.drop(columns=[target])\n",
    "    y_test = test_df[[target]]\n",
    "\n",
    "    train_ds = TabularDataset(\n",
    "        X_train, y_train, sensitive_features=sensitive_features, drop_columns=drop_columns)\n",
    "    test_ds = TabularDataset(\n",
    "        X_test, y_test, sensitive_features=sensitive_features, drop_columns=drop_columns)\n",
    "\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a59a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREDIT DATASET:\n",
    "# This research employed a binary variable, default payment (Yes = 1, No = 0), as the response\n",
    "# variable. This study reviewed the literature and used the following 23 variables as explanatory\n",
    "# variables:\n",
    "#     x1: Amount of the given credit (NT dollar): it includes both the individual consumer\n",
    "#         credit and his/her family (supplementary) credit.\n",
    "#     x2: Gender (1 = male; 2 = female).\n",
    "#     x3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
    "#     x4: Marital status (1 = married; 2 = single; 3 = others).\n",
    "#     x5: Age (year).\n",
    "#     x6 - x11: History of past payment. We tracked the past monthly payment records (from April to\n",
    "#         September, 2005) as follows: x6 = the repayment status in September, 2005; x7 = the\n",
    "#         repayment status in August, 2005; . . .;x11 = the repayment status in April, 2005. The\n",
    "#         measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one\n",
    "#         month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months;\n",
    "#         9 = payment delay for nine months and above.\n",
    "#     x12-x17: Amount of bill statement (NT dollar). x12 = amount of bill statement in September,\n",
    "#         2005; x13 = amount of bill statement in August, 2005; . . .; x17 = amount of bill\n",
    "#         statement in April, 2005.\n",
    "#     x18-x23: Amount of previous payment (NT dollar). x18 = amount paid in September, 2005;\n",
    "#         x19 = amount paid in August, 2005; . . .;x23 = amount paid in April, 2005.\n",
    "def get_credit_data(sensitive_features, drop_columns=[], test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    sensitive_features: features that should be considered sensitive when building the\n",
    "        BiasedDataset object\n",
    "    drop_columns: columns we can ignore and drop\n",
    "    random_state: to pass to train_test_split\n",
    "    return: two BiasedDataset objects, for training and test data respectively\n",
    "    \"\"\"\n",
    "    credit_data = fetch_openml(data_id=42477, as_frame=True, data_home='./data/raw')\n",
    "\n",
    "    # Force categorical data do be dtype: category\n",
    "    features = credit_data.data\n",
    "    categorical_features = ['x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11']\n",
    "    for cf in categorical_features:\n",
    "        features[cf] = features[cf].astype(str).astype('category')\n",
    "\n",
    "    # Encode output\n",
    "    target = (credit_data.target == \"1\") * 1\n",
    "    target = pd.DataFrame({'target': target})\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    train_ds = TabularDataset(\n",
    "        X_train, y_train, sensitive_features=sensitive_features, drop_columns=drop_columns)\n",
    "    test_ds = TabularDataset(\n",
    "        X_test, y_test, sensitive_features=sensitive_features, drop_columns=drop_columns)\n",
    "\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd9a1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crime_data(sensitive_features, drop_columns=[], test_size=0.2, random_state=42):\n",
    "    data_df = pd.read_csv(DATA_CRIME_FILENAME, na_values='?').dropna()\n",
    "    train_df, test_df = train_test_split(data_df, test_size=test_size, random_state=random_state)\n",
    "    target = 'ViolentCrimesPerPop'\n",
    "\n",
    "    X_train = train_df.drop(columns=[target])\n",
    "    y_train = train_df[[target]]\n",
    "    X_test = test_df.drop(columns=[target])\n",
    "    y_test = test_df[[target]]\n",
    "\n",
    "    train_ds = TabularDataset(\n",
    "        X_train, y_train, sensitive_features=sensitive_features, drop_columns=drop_columns)\n",
    "    test_ds = TabularDataset(\n",
    "        X_test, y_test, sensitive_features=sensitive_features, drop_columns=drop_columns)\n",
    "\n",
    "    return train_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df29da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_german_data(sensitive_features, drop_columns=[], test_size=0.2, random_state=42):\n",
    "    data_df = pd.read_csv(DATA_GERMAN_FILENAME, na_values='?').dropna()\n",
    "\n",
    "    train_df, test_df = train_test_split(data_df, test_size=test_size, random_state=random_state)\n",
    "    target = 'target'\n",
    "\n",
    "    X_train = train_df.drop(columns=[target])\n",
    "    y_train = train_df[[target]]\n",
    "    X_test = test_df.drop(columns=[target])\n",
    "    y_test = test_df[[target]]\n",
    "\n",
    "    train_ds = TabularDataset(\n",
    "        X_train, y_train, sensitive_features=sensitive_features, drop_columns=drop_columns)\n",
    "    test_ds = TabularDataset(\n",
    "        X_test, y_test, sensitive_features=sensitive_features, drop_columns=drop_columns)\n",
    "\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c582c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_features = ['sex', 'race']\n",
    "drop_columns = ['native-country', 'education']\n",
    "        \n",
    "train_ds, test_ds = get_adult_data(sensitive_features, drop_columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da56461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/XAI_Turing/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sensitive_features = ['x2']\n",
    "drop_columns = []\n",
    "train_ds, test_ds = get_credit_data(sensitive_features, drop_columns=drop_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9714853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sensitive_features = ['status_sex']\n",
    "drop_columns = []\n",
    "train_ds, test_ds = get_german_data(sensitive_features, drop_columns=drop_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de8d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRIME_DROP_COLUMNS = [\n",
    "    'HispPerCap', 'LandArea', 'LemasPctOfficDrugUn', 'MalePctNevMarr',\n",
    "    'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'MedRent',\n",
    "    'MedYrHousBuilt', 'OwnOccHiQuart', 'OwnOccLowQuart',\n",
    "    'OwnOccMedVal', 'PctBornSameState', 'PctEmplManu',\n",
    "    'PctEmplProfServ', 'PctEmploy', 'PctForeignBorn', 'PctImmigRec5',\n",
    "    'PctImmigRec8', 'PctImmigRecent', 'PctRecImmig10', 'PctRecImmig5',\n",
    "    'PctRecImmig8', 'PctRecentImmig', 'PctSameCity85',\n",
    "    'PctSameState85', 'PctSpeakEnglOnly', 'PctUsePubTrans',\n",
    "    'PctVacMore6Mos', 'PctWorkMom', 'PctWorkMomYoungKids',\n",
    "    'PersPerFam', 'PersPerOccupHous', 'PersPerOwnOccHous',\n",
    "    'PersPerRentOccHous', 'RentHighQ', 'RentLowQ', 'Unnamed: 0',\n",
    "    'agePct12t21', 'agePct65up', 'householdsize', 'indianPerCap',\n",
    "    'pctUrban', 'pctWFarmSelf', 'pctWRetire', 'pctWSocSec', 'pctWWage',\n",
    "    'whitePerCap'\n",
    "]\n",
    "sensitive_features = ['racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp']\n",
    "train_ds, test_ds = get_crime_data(sensitive_features, drop_columns=CRIME_DROP_COLUMNS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
