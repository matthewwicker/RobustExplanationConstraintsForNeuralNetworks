{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98514f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a102cab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  48842\n"
     ]
    }
   ],
   "source": [
    "\n",
    "full_data = pd.read_csv(\n",
    "    \"./Datasets/adult.csv\",\n",
    "    names=[\n",
    "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"],\n",
    "        sep=r'\\s*,\\s*',\n",
    "        engine='python', skiprows=1,\n",
    "        na_values=\"?\", dtype={0:int, 1:str, 2:int, 3:str, 4:int, 5: str, 6:str , 7:str ,8:str ,9: str, 10:int, 11:int, 12:int, 13:str,14: str})\n",
    "\n",
    "print('Dataset size: ', full_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9dc0849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size Before pruning:  48842\n",
      "Dataset size after pruning:  45222\n",
      "We eliminated  3620  datapoints\n"
     ]
    }
   ],
   "source": [
    "str_list=[]\n",
    "for data in [full_data]:\n",
    "    for colname, colvalue in data.iteritems(): \n",
    "        if type(colvalue[1]) == str:\n",
    "            str_list.append(colname) \n",
    "num_list = data.columns.difference(str_list)\n",
    "\n",
    "full_size = full_data.shape[0]\n",
    "print('Dataset size Before pruning: ', full_size)\n",
    "for data in [full_data]:\n",
    "    for i in full_data:\n",
    "        data[i].replace('nan', np.nan, inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "real_size = full_data.shape[0]\n",
    "print('Dataset size after pruning: ', real_size)\n",
    "print('We eliminated ', (full_size-real_size), ' datapoints')\n",
    "\n",
    "# Take\n",
    "full_labels = full_data['Target'].copy()\n",
    "full_data = full_data.drop(['Target'], axis=1)\n",
    "\n",
    "# Label Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "full_labels = label_encoder.fit_transform(full_labels)\n",
    "\n",
    "# Segment categorical and non categorical data (will manipulate cat_data, and append them back later)\n",
    "cat_data = full_data.select_dtypes(include=['object']).copy()\n",
    "other_data = full_data.select_dtypes(include=['int']).copy()\n",
    "\n",
    "newcat_data = pd.get_dummies(cat_data, columns=[\n",
    "    \"Workclass\", \"Education\", \"Country\" ,\"Relationship\", \"Martial Status\", \"Occupation\", \"Relationship\",\n",
    "    \"Race\", \"Sex\"\n",
    "])\n",
    "\n",
    "\n",
    "full_data = pd.concat([other_data, newcat_data], axis=1)\n",
    "\n",
    "train_size = 30000\n",
    "valid_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec42450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 110)\n",
      "(30000,)\n",
      "\n",
      "(10000, 110)\n",
      "(10000,)\n",
      "\n",
      "(5222, 110)\n",
      "(5222,)\n"
     ]
    }
   ],
   "source": [
    "train_x = full_data.iloc[:train_size, :].to_numpy()\n",
    "train_y = full_labels[:train_size]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print()\n",
    "\n",
    "valid_x = full_data.iloc[train_size:(train_size+valid_size), :].to_numpy()\n",
    "valid_y = full_labels[train_size:(train_size+valid_size)]\n",
    "print(valid_x.shape)\n",
    "print(valid_y.shape)\n",
    "print()\n",
    "\n",
    "test_x = full_data.iloc[(train_size+valid_size):, :].to_numpy()\n",
    "test_y = full_labels[(train_size+valid_size):]\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "num_features = test_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184b5a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7536cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import GradCertModule\n",
    "import XAIArchitectures\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class custDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(X).float()\n",
    "        self.y = y\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "\n",
    "AdultTrain = custDataset(train_x, train_y)    \n",
    "AdultVal = custDataset(valid_x, valid_y) \n",
    "AdultTest = custDataset(test_x, test_y)\n",
    "\n",
    "class AdultDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train, val, test, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.train_data = train\n",
    "        self.val_data = val\n",
    "        self.test_data = test\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size)\n",
    "\n",
    "    #def predict_dataloader(self):\n",
    "    #    return DataLoader(self.mnist_predict, batch_size=self.batch_size)\n",
    "    \n",
    "dm = AdultDataModule(AdultTrain, AdultVal, AdultTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "685aebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1.0            # Regularization Parameter (Weights the Reg. Term)\n",
    "EPSILON = 1000          # Input Peturbation Budget at Training Time\n",
    "GAMMA = 0.0            # Model Peturbation Budget at Training Time \n",
    "                       #(Changed to proportional budget rather than absolute)\n",
    "    \n",
    "LEARN_RATE = 0.00001     # Learning Rate Hyperparameter\n",
    "HIDDEN_DIM = 128       # Hidden Neurons Hyperparameter\n",
    "HIDDEN_LAY = 1         # Hidden Layers Hyperparameter\n",
    "MAX_EPOCHS = 10\n",
    "\n",
    "EPSILON_LINEAR = True   # Put Epsilon on a Linear Schedule?\n",
    "GAMMA_LINEAR = True     # Put Gamma on a Linear Schedule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6454ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = XAIArchitectures.FullyConnected(hidden_dim=HIDDEN_DIM, hidden_lay=HIDDEN_LAY, dataset=\"ADULT\")\n",
    "model.set_params(alpha=ALPHA, epsilon=EPSILON, gamma=GAMMA, \n",
    "                learn_rate=LEARN_RATE, max_epochs=MAX_EPOCHS,\n",
    "                epsilon_linear=EPSILON_LINEAR, gamma_linear=GAMMA_LINEAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "759fbb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | lays | ModuleList | 14.5 K\n",
      "1 | l1   | Linear     | 14.2 K\n",
      "2 | lf   | Linear     | 258   \n",
      "------------------------------------\n",
      "14.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.5 K    Total params\n",
      "0.058     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/XAI_Turing/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "/usr/local/anaconda3/envs/XAI_Turing/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (_ResultMetric). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "/usr/local/anaconda3/envs/XAI_Turing/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70524beca11f499d90aeb8954e87ef26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/XAI_Turing/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "/usr/local/anaconda3/envs/XAI_Turing/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d980b2e9fe57414c9daf36e113be6cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=MAX_EPOCHS, accelerator=\"cpu\", devices=1)\n",
    "trainer.fit(model, datamodule=dm)\n",
    "result = trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2293f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = \"Models\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "SCHEDULED = model.EPSILON_LINEAR or model.GAMMA_LINEAR\n",
    "MODEL_ID = \"FCN_e=%s_g=%s_a=%s_l=%s_h=%s_s=%s\"%(model.EPSILON, model.GAMMA, model.ALPHA, HIDDEN_LAY, HIDDEN_DIM, SCHEDULED)\n",
    "trainer.save_checkpoint(\"Models/%s.ckpt\"%(MODEL_ID))\n",
    "torch.save(model.state_dict(), \"Models/%s.pt\"%(MODEL_ID))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
