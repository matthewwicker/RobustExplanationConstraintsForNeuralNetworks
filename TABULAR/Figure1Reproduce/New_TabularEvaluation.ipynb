{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11324e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Imports\n",
    "from typing import Union\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "import data_utils\n",
    "import GradCertModule\n",
    "import XAIArchitectures\n",
    "# Deep Learning Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "import pytorch_lightning as pl\n",
    "# Standard Lib Imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 0\n",
    "import numpy as np\n",
    "import random\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "dataset = \"ADULT\"\n",
    "\n",
    "if(dataset == \"GERMAN\"):\n",
    "    negative_cls = 0\n",
    "    sensitive_features = [] \n",
    "    sens = ['status_sex_A91', 'status_sex_A92', 'status_sex_A93', 'status_sex_A94', 'status_sex_A95']\n",
    "    drop_columns = []\n",
    "    train_ds, test_ds = data_utils.get_german_data(sensitive_features, drop_columns=drop_columns)\n",
    "\n",
    "elif(dataset == \"CREDIT\"):\n",
    "    negative_cls = 1\n",
    "    sensitive_features = [] \n",
    "    sens = ['x2_1.0', 'x2_2.0']\n",
    "    drop_columns = []\n",
    "    train_ds, test_ds = data_utils.get_credit_data(sensitive_features, drop_columns=drop_columns)\n",
    "    \n",
    "elif(dataset == \"ADULT\"):\n",
    "    negative_cls = 1\n",
    "    sensitive_features = [] \n",
    "    sens = ['sex_Female', 'sex_Male', 'race_Amer-Indian-Eskimo', \n",
    "            'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White',]\n",
    "    drop_columns = ['native-country'] #, 'education']\n",
    "    train_ds, test_ds = data_utils.get_adult_data(sensitive_features, drop_columns=drop_columns)\n",
    "    \n",
    "elif(dataset == \"CRIME\"):\n",
    "    negative_cls = 1\n",
    "    CRIME_DROP_COLUMNS = [\n",
    "    'HispPerCap', 'LandArea', 'LemasPctOfficDrugUn', 'MalePctNevMarr',\n",
    "    'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'MedRent',\n",
    "    'MedYrHousBuilt', 'OwnOccHiQuart', 'OwnOccLowQuart',\n",
    "    'OwnOccMedVal', 'PctBornSameState', 'PctEmplManu',\n",
    "    'PctEmplProfServ', 'PctEmploy', 'PctForeignBorn', 'PctImmigRec5',\n",
    "    'PctImmigRec8', 'PctImmigRecent', 'PctRecImmig10', 'PctRecImmig5',\n",
    "    'PctRecImmig8', 'PctRecentImmig', 'PctSameCity85',\n",
    "    'PctSameState85', 'PctSpeakEnglOnly', 'PctUsePubTrans',\n",
    "    'PctVacMore6Mos', 'PctWorkMom', 'PctWorkMomYoungKids',\n",
    "    'PersPerFam', 'PersPerOccupHous', 'PersPerOwnOccHous',\n",
    "    'PersPerRentOccHous', 'RentHighQ', 'RentLowQ', 'Unnamed: 0',\n",
    "    'agePct12t21', 'agePct65up', 'householdsize', 'indianPerCap',\n",
    "    'pctUrban', 'pctWFarmSelf', 'pctWRetire', 'pctWSocSec', 'pctWWage',\n",
    "    'whitePerCap'\n",
    "    ]\n",
    "    sensitive_features = []\n",
    "    sens = ['racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp']\n",
    "    train_ds, test_ds = data_utils.get_crime_data(sensitive_features, drop_columns=CRIME_DROP_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a03dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'capital-gain', 'capital-loss', 'education-num', 'fnlwgt', 'hours-per-week', 'education_10th', 'education_11th', 'education_12th', 'education_1st-4th', 'education_5th-6th', 'education_7th-8th', 'education_9th', 'education_Assoc-acdm', 'education_Assoc-voc', 'education_Bachelors', 'education_Doctorate', 'education_HS-grad', 'education_Masters', 'education_Preschool', 'education_Prof-school', 'education_Some-college', 'marital-status_Divorced', 'marital-status_Married-AF-spouse', 'marital-status_Married-civ-spouse', 'marital-status_Married-spouse-absent', 'marital-status_Never-married', 'marital-status_Separated', 'marital-status_Widowed', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White', 'relationship_Husband', 'relationship_Not-in-family', 'relationship_Other-relative', 'relationship_Own-child', 'relationship_Unmarried', 'relationship_Wife', 'sex_Female', 'sex_Male', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay']\n"
     ]
    }
   ],
   "source": [
    "#print(train_ds.X_df.columns.tolist())\n",
    "cols = train_ds.X_df.columns.tolist()\n",
    "print(cols)\n",
    "sens_inds = []\n",
    "for i in sens:\n",
    "    sens_inds.append(cols.index(i))\n",
    "#print(sens_inds)\n",
    "\n",
    "if(dataset == \"ADULT\"):\n",
    "    AGE = [cols.index('age')]\n",
    "    RACE = [cols.index(i) for i in ['race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White']]\n",
    "    GENDER = [cols.index(i) for i in ['sex_Female', 'sex_Male']]\n",
    "    FINANCES = [cols.index(i) for i in ['capital-gain', 'capital-loss']]\n",
    "    EDUCATION = [cols.index(i) for i in ['education_10th', 'education_11th', 'education_12th', 'education_1st-4th', 'education_5th-6th', 'education_7th-8th', 'education_9th', \n",
    "                                         'education_Assoc-acdm', 'education_Assoc-voc', 'education_Bachelors', 'education_Doctorate', 'education_HS-grad', 'education_Masters',\n",
    "                                         'education_Preschool', 'education_Prof-school', 'education_Some-college']]     \n",
    "    EMPLOYMENT = [cols.index(i) for i in ['hours-per-week', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', \n",
    "                                          'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', \n",
    "                                          'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'workclass_Local-gov', 'workclass_Private',\n",
    "                                          'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay']]          \n",
    "    PERSONAL = [cols.index(i) for i in ['hours-per-week', 'relationship_Husband', 'relationship_Not-in-family', 'relationship_Other-relative', 'relationship_Own-child', 'relationship_Unmarried', 'relationship_Wife', 'marital-status_Divorced',\n",
    "                                        'marital-status_Married-AF-spouse', 'marital-status_Married-civ-spouse', 'marital-status_Married-spouse-absent', 'marital-status_Never-married', 'marital-status_Separated', 'marital-status_Widowed']]\n",
    "                           \n",
    "elif(dataset == \"CREDIT\"):\n",
    "    AMOUNT = [cols.index('x1')]\n",
    "    AGE = [cols.index(i) for i in [ 'x5_21.0', 'x5_22.0', 'x5_23.0', 'x5_24.0', 'x5_25.0', 'x5_26.0', 'x5_27.0', 'x5_28.0', 'x5_29.0', 'x5_30.0', 'x5_31.0', \n",
    "                                   'x5_32.0', 'x5_33.0', 'x5_34.0', 'x5_35.0', 'x5_36.0', 'x5_37.0', 'x5_38.0', 'x5_39.0', 'x5_40.0', 'x5_41.0', 'x5_42.0', \n",
    "                                   'x5_43.0', 'x5_44.0', 'x5_45.0', 'x5_46.0', 'x5_47.0', 'x5_48.0', 'x5_49.0', 'x5_50.0', 'x5_51.0', 'x5_52.0', 'x5_53.0', \n",
    "                                   'x5_54.0', 'x5_55.0', 'x5_56.0', 'x5_57.0', 'x5_58.0', 'x5_59.0', 'x5_60.0', 'x5_61.0', 'x5_62.0', 'x5_63.0', 'x5_64.0', \n",
    "                                   'x5_65.0', 'x5_66.0', 'x5_67.0', 'x5_68.0', 'x5_69.0', 'x5_70.0', 'x5_71.0', 'x5_72.0', 'x5_73.0', 'x5_74.0', 'x5_75.0', \n",
    "                                   'x5_79.0']]\n",
    "    GENDER = [cols.index(i) for i in ['x2_1.0', 'x2_2.0']]\n",
    "    EDUCATION = [cols.index(i) for i in ['x2_1.0', 'x2_2.0']]\n",
    "    PERSONAL = [cols.index(i) for i in ['x4_0.0', 'x4_1.0', 'x4_2.0', 'x4_3.0']]\n",
    "    BILLS = [cols.index(i) for i in ['x12', 'x13', 'x14', 'x15', 'x16', 'x17']]\n",
    "    PAYMENTS = [cols.index(i) for i in ['x6_-1.0', 'x6_-2.0', 'x6_0.0', 'x6_1.0', 'x6_2.0', 'x6_3.0', 'x6_4.0', 'x6_5.0', 'x6_6.0', 'x6_7.0', 'x6_8.0', \n",
    "                                        'x7_-1.0', 'x7_-2.0', 'x7_0.0', 'x7_1.0', 'x7_2.0', 'x7_3.0', 'x7_4.0', 'x7_5.0', 'x7_6.0', 'x7_7.0', 'x7_8.0', \n",
    "                                        'x8_-1.0', 'x8_-2.0', 'x8_0.0', 'x8_1.0', 'x8_2.0', 'x8_3.0', 'x8_4.0', 'x8_5.0', 'x8_6.0', 'x8_7.0', 'x8_8.0', \n",
    "                                        'x9_-1.0', 'x9_-2.0', 'x9_0.0', 'x9_1.0', 'x9_2.0', 'x9_3.0', 'x9_4.0', 'x9_5.0', 'x9_6.0', 'x9_7.0', 'x9_8.0', \n",
    "                                        'x10_-1.0', 'x10_-2.0', 'x10_0.0', 'x10_2.0', 'x10_3.0', 'x10_4.0', 'x10_5.0', 'x10_6.0', 'x10_7.0', 'x10_8.0', \n",
    "                                        'x11_-1.0', 'x11_-2.0', 'x11_0.0', 'x11_2.0', 'x11_3.0', 'x11_4.0', 'x11_5.0', 'x11_6.0', 'x11_7.0', 'x11_8.0',\n",
    "                                        'x18', 'x19', 'x20', 'x21', 'x22', 'x23']]\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f48a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_ds.X_df.to_numpy()\n",
    "y_train = torch.squeeze(torch.Tensor(train_ds.y_df.to_numpy()).to(torch.int64))\n",
    "\n",
    "X_test = test_ds.X_df.to_numpy()\n",
    "y_test = torch.squeeze(torch.Tensor(test_ds.y_df.to_numpy()).to(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1d88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class custDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(X).float()\n",
    "        self.y = y\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "\n",
    "CustTrain = custDataset(X_train, y_train)    \n",
    "CustTest = custDataset(X_test, y_test)\n",
    "\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train, val, test, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.train_data = train\n",
    "        self.val_data = val\n",
    "        self.test_data = test\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size)\n",
    "    \n",
    "dm = CustomDataModule(CustTrain, CustTest, CustTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f3c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1.0            # Regularization Parameter (Weights the Reg. Term)\n",
    "EPSILON = 0.0          # Input Peturbation Budget at Training Time\n",
    "GAMMA = 0.0            # Model Peturbation Budget at Training Time \n",
    "                       #(Changed to proportional budget rather than absolute)\n",
    "    \n",
    "LEARN_RATE = 0.0005     # Learning Rate Hyperparameter\n",
    "HIDDEN_DIM = 256       # Hidden Neurons Hyperparameter\n",
    "HIDDEN_LAY = 2         # Hidden Layers Hyperparameter\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "EPSILON_LINEAR = True   # Put Epsilon on a Linear Schedule?\n",
    "GAMMA_LINEAR = True     # Put Gamma on a Linear Schedule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca09bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SET MODE TO:  GRAD\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = XAIArchitectures.FullyConnected(hidden_dim=HIDDEN_DIM, hidden_lay=HIDDEN_LAY, dataset=dataset)\n",
    "model.set_params(alpha=ALPHA, epsilon=EPSILON, gamma=GAMMA, \n",
    "                learn_rate=LEARN_RATE, max_epochs=MAX_EPOCHS,\n",
    "                epsilon_linear=EPSILON_LINEAR, gamma_linear=GAMMA_LINEAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15072302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\n",
      "epoch\n",
      "global_step\n",
      "pytorch-lightning_version\n",
      "state_dict\n",
      "loops\n",
      "callbacks\n",
      "optimizer_states\n",
      "lr_schedulers\n",
      "hparams_name\n",
      "hyper_parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "SCHEDULED = EPSILON_LINEAR or GAMMA_LINEAR    \n",
    "#MODEL_ID = \"FCN_e=%s_g=%s_h=%s_l=%s_s=%s\"%(EPSILON, GAMMA, HIDDEN_DIM, HIDDEN_LAY, SCHEDULED)  \n",
    "MODEL_ID = \"%s_FCN_e=%s_g=%s_a=%s_l=%s_h=%s_s=%s\"%(dataset, EPSILON, GAMMA, ALPHA, HIDDEN_LAY, HIDDEN_DIM, SCHEDULED)\n",
    "print(MODEL_ID)\n",
    "ckpt = torch.load(\"Models/%s.ckpt\"%(MODEL_ID))\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "checkpoint = torch.load(\"Models/%s.ckpt\"%(MODEL_ID))\n",
    "for key in checkpoint:\n",
    "    print(key)\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c964cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewwicker/opt/anaconda3/envs/XAIenvironment/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484744261/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "# Measure Test Set Accuracy\n",
    "def get_test_acc(MODEL_ID):\n",
    "    correct = 0\n",
    "    for INDEX in range(len(X_test)):\n",
    "        data = torch.Tensor([X_test[INDEX]])\n",
    "        out, cls = model.classify(data)\n",
    "        if(cls == y_test[INDEX]):\n",
    "            correct += 1 \n",
    "    correct /= len(X_test)\n",
    "    #print(\"Test set accuracy: \", correct)\n",
    "    return correct\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "std_acc = get_test_acc(M_ID)\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "rob_acc = get_test_acc(M_ID)\n",
    "\n",
    "M_ID = \"ADULT_L2_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "el2_acc = get_test_acc(M_ID)\n",
    "\n",
    "M_ID = \"ADULT_HESSIAN_FCN_e=0.05_g=0.0_a=0.1_l=2_h=256_s=True\"\n",
    "hes_acc = get_test_acc(M_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc5689b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                    | 0/30 [00:00<?, ?it/s]../../GradCertModule.py:695: UserWarning: Using a target size (torch.Size([63])) that is different to the input size (torch.Size([1, 63])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_expl = F.mse_loss(grad_adv, target_expl)\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 30/30 [00:05<00:00,  5.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 30/30 [00:05<00:00,  5.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 30/30 [00:05<00:00,  5.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 30/30 [00:05<00:00,  5.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Measure Input Attack Robustness\n",
    "from tqdm import trange\n",
    "def get_input_attk(MODEL_ID, EPS=0.5, N=10):\n",
    "    model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n",
    "    model.inputfooling_ON()\n",
    "    fooled = 0\n",
    "    for INDEX in trange(N):\n",
    "        success, x_adv, grad_adv = GradCertModule.run_tabular_attack(model, torch.Tensor(X_test[INDEX]), iterations=100,\n",
    "                                                      target=sens_inds, epsilon=EPS, lr=0.05)\n",
    "        fooled += int(success)\n",
    "        #print(success)\n",
    "    model.inputfooling_OFF()\n",
    "    return fooled/N\n",
    "    #print(\"Input Attack Fooling Rate: \", fooled/5)\n",
    "    \n",
    "   \n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "std_targ_inp_attk = get_input_attk(MODEL_ID, EPS=0.2, N=30)\n",
    "\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "rob_targ_inp_attk = get_input_attk(M_ID, EPS=0.25, N=30)\n",
    "\n",
    "M_ID = \"ADULT_L2_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "el2_targ_inp_attk = get_input_attk(M_ID, EPS=0.25, N=30)\n",
    "\n",
    "M_ID = \"ADULT_HESSIAN_FCN_e=0.05_g=0.0_a=0.1_l=2_h=256_s=True\"\n",
    "hes_targ_inp_attk = get_input_attk(M_ID, EPS=0.25, N=30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6cfdfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                    | 0/30 [00:00<?, ?it/s]../../GradCertModule.py:780: UserWarning: Using a target size (torch.Size([63])) that is different to the input size (torch.Size([1, 63])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_expl = F.mse_loss(grad_adv, target_expl)\n",
      " 57%|██████████████████████████████████████████▌                                | 17/30 [00:16<00:12,  1.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wk/dw_2kpzx6yggmhdyw5k5gj200000gn/T/ipykernel_17343/3405648922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mM_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mstd_targ_mod_attk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_attk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mM_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ADULT_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wk/dw_2kpzx6yggmhdyw5k5gj200000gn/T/ipykernel_17343/3405648922.py\u001b[0m in \u001b[0;36mget_model_attk\u001b[0;34m(MODEL_ID, GAM, N)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#                                              target=sens_inds, gamma=GAM, lr=0.005*GAM, idx=7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         success, x_adv, grad_adv = GradCertModule.run_tabular_model_attack(model, torch.Tensor(X_test[INDEX]), iterations=150,\n\u001b[0;32m---> 10\u001b[0;31m                                                       target=sens_inds, gamma=GAM, lr=0.02, idx=7) \n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Development/CertifiedExplanations/GradCertModule.py\u001b[0m in \u001b[0;36mrun_tabular_model_attack\u001b[0;34m(model, x, target, gamma, iterations, lr, idx, tau)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;31m#optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0my_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0mgrad_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;31m#print(grad_adv, value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Development/CertifiedExplanations/GradCertModule.py\u001b[0m in \u001b[0;36mget_gradient\u001b[0;34m(model, x, desired_index)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mprefactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_summands\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_summands\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mparallel_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefactors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_summands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0;31m# we sum the result and then take the derivative (instead of summing derivatives as in most implementations),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;31m# (d/dx) (n*y_1(1/n*x) + n/2*y_1(2/n*x) .... + y_n(x) ) = y_1'+....y'_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/XAIenvironment/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/XAIenvironment/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataParallel.forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/XAIenvironment/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Development/CertifiedExplanations/XAIArchitectures/FullyConnected.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputfooling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Measure Model Attack Robustness\n",
    "def get_model_attk(MODEL_ID, GAM=0.5, N=10):\n",
    "    fooled = 0\n",
    "    for INDEX in trange(N):\n",
    "        model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n",
    "        model.inputfooling_ON()\n",
    "        #success, x_adv, grad_adv = GradCertModule.run_tabular_model_attack(model, torch.Tensor(X_test[INDEX]), iterations=150,\n",
    "        #                                              target=sens_inds, gamma=GAM, lr=0.005*GAM, idx=7) \n",
    "        success, x_adv, grad_adv = GradCertModule.run_tabular_model_attack(model, torch.Tensor(X_test[INDEX]), iterations=150,\n",
    "                                                      target=sens_inds, gamma=GAM, lr=0.02, idx=7) \n",
    "        \n",
    "        \n",
    "        #print(grad_adv)\n",
    "        fooled += int(success)\n",
    "    model.inputfooling_OFF()  \n",
    "    return fooled/N\n",
    "    #print(\"Model Attack Fooling Rate: \", fooled/5)\n",
    "\n",
    "\n",
    "\n",
    "#get_model_attk(MODEL_ID, GAM=0.01, N=10)\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "std_targ_mod_attk = get_model_attk(M_ID, GAM=0.2, N=30)\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "rob_targ_mod_attk = get_model_attk(M_ID, GAM=0.25, N=30)\n",
    "\n",
    "M_ID = \"ADULT_L2_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "el2_targ_mod_attk = get_model_attk(M_ID, GAM=0.25, N=30)\n",
    "\n",
    "M_ID = \"ADULT_HESSIAN_FCN_e=0.05_g=0.0_a=0.1_l=2_h=256_s=True\"\n",
    "hes_targ_mod_attk = get_model_attk(M_ID, GAM=0.25, N=30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b42038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_cert(MODEL_ID, EPS=0.2):\n",
    "    model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n",
    "    import copy\n",
    "    certified = 0\n",
    "    for INDEX in trange(200):\n",
    "        lower, upper = GradCertModule.GradCertBounds(model, torch.Tensor(X_test[INDEX][None, :]),\n",
    "                                                     y_test[INDEX], eps=EPS, gam=0.00, nclasses=2)\n",
    "\n",
    "        upper = np.squeeze(upper.detach().numpy())\n",
    "        lower = np.squeeze(lower.detach().numpy())\n",
    "        #print(upper[sens_inds])\n",
    "        #print(lower[sens_inds])\n",
    "        temp = copy.deepcopy(lower)\n",
    "        for i in sens_inds:\n",
    "            temp[i] = upper[i]\n",
    "        #print(temp[sens_inds])\n",
    "        top_idx = np.squeeze(np.argsort(temp))\n",
    "        top_idx = list(reversed(top_idx))\n",
    "        #print(set(top_idx[0:10]))\n",
    "        #print( set(sens_inds))\n",
    "        cert = not bool(set(top_idx[0:5]) & set(sens_inds))\n",
    "        certified += int(cert)\n",
    "        #break\n",
    "    #print(\"Input Attack Certified: \", certified/200)\n",
    "    print(certified/200)\n",
    "    return certified/200\n",
    "\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "std_targ_inp_cert = get_input_cert(MODEL_ID, EPS=0.05)\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "rob_targ_inp_cert = get_input_cert(M_ID, EPS=0.05)\n",
    "\n",
    "M_ID = \"ADULT_L2_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "el2_targ_inp_cert = get_input_cert(M_ID, EPS=0.05)\n",
    "\n",
    "M_ID = \"ADULT_HESSIAN_FCN_e=0.05_g=0.0_a=0.1_l=2_h=256_s=True\"\n",
    "hes_targ_inp_cert = get_input_cert(M_ID, EPS=0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95828ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "\n",
    "colors = sns.color_palette(\"Set2\", 4)\n",
    "\n",
    "std_vals = np.asarray([std_acc, 1-std_targ_inp_attk, std_targ_inp_cert])\n",
    "std_vals = np.clip(std_vals, 0, 1)\n",
    "std_vals = np.round(std_vals,2)\n",
    "\n",
    "rob_vals = np.asarray([rob_acc, 1-rob_targ_inp_attk, rob_targ_inp_cert])\n",
    "rob_vals = np.clip(rob_vals, 0, 1)\n",
    "rob_vals = np.round(rob_vals,2)\n",
    "\n",
    "el2_vals = np.asarray([el2_acc, 1-el2_targ_inp_attk, el2_targ_inp_cert])\n",
    "el2_vals = np.clip(el2_vals, 0, 1)\n",
    "el2_vals = np.round(el2_vals,2)\n",
    "\n",
    "hes_vals = np.asarray([hes_acc, 1-hes_targ_inp_attk, hes_targ_inp_cert])\n",
    "hes_vals = np.clip(hes_vals, 0, 1)\n",
    "hes_vals = np.round(hes_vals,2)\n",
    "\n",
    "x = np.arange(len(std_vals)) # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,7), dpi=200)\n",
    "rects1 = ax.bar(x - width*2, std_vals, width, label='Standard', color=colors[3], )\n",
    "rects2 = ax.bar(x - width, rob_vals, width, label='Grad Cert.', color=colors[0])\n",
    "rects3 = ax.bar(x + 0, el2_vals, width, label='L2 Reg.', color=colors[2])\n",
    "rects4 = ax.bar(x + width, hes_vals, width, label='Hess. Reg.', color=colors[1])\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "labels = [\"(Accuracy)\", \"(Attack Robustness)\", \"(Certified Robustness)\"]\n",
    "ax.set_xticks(x, labels, size=20)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=40, label_type='center', rotation = 90, size=22)\n",
    "ax.bar_label(rects2, padding=40, label_type='center',  rotation = 90, size=22)\n",
    "ax.bar_label(rects3, padding=40, label_type='center', rotation = 90, size=22)\n",
    "ax.bar_label(rects4, padding=40, label_type='center',  rotation = 90, size=22)\n",
    "\n",
    "plt.ylim((0,1.05))\n",
    "fig.tight_layout()\n",
    "plt.ylabel('Robustness (/Accuracy)')\n",
    "plt.title(\"Adult - Robustness Against Fooling\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc26ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Input Attack Certification\n",
    "def get_model_cert(MODEL_ID, GAM=0.2):\n",
    "    model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n",
    "    import copy\n",
    "    certified = 0\n",
    "    for INDEX in trange(200):\n",
    "        lower, upper = GradCertModule.GradCertBounds(model, torch.Tensor(X_test[INDEX][None, :]),\n",
    "                                                     y_test[INDEX], eps=0.00, gam=GAM, nclasses=2)\n",
    "\n",
    "        upper = np.squeeze(upper.detach().numpy())\n",
    "        lower = np.squeeze(lower.detach().numpy())\n",
    "        #print(upper[sens_inds])\n",
    "        #print(lower[sens_inds])\n",
    "        temp = copy.deepcopy(lower)\n",
    "        for i in sens_inds:\n",
    "            temp[i] = upper[i]\n",
    "        #print(temp[sens_inds])\n",
    "        top_idx = np.squeeze(np.argsort(temp))\n",
    "        top_idx = list(reversed(top_idx))\n",
    "        #print(set(top_idx[0:10]))\n",
    "        #print( set(sens_inds))\n",
    "        cert = not bool(set(top_idx[0:5]) & set(sens_inds))\n",
    "        certified += int(cert)\n",
    "        #break\n",
    "    print(certified/200)\n",
    "    return certified/200\n",
    "\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "std_targ_mod_cert = get_model_cert(MODEL_ID, GAM=0.03)\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.05_a=1.0_l=2_h=256_s=True\"\n",
    "rob_targ_mod_cert = get_model_cert(M_ID, GAM=0.03)\n",
    "\n",
    "M_ID = \"ADULT_L2_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "el2_targ_mod_cert = get_model_cert(M_ID, GAM=0.03)\n",
    "\n",
    "M_ID = \"ADULT_HESSIAN_FCN_e=0.05_g=0.0_a=0.1_l=2_h=256_s=True\"\n",
    "hes_targ_mod_cert = get_model_cert(M_ID, GAM=0.03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ad50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untargeted Input Attack \n",
    "\n",
    "# Measure Input Attack Robustness\n",
    "from tqdm import trange\n",
    "def get_untarg_attk(MODEL_ID, EPS=0.0, N=10):\n",
    "    model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n",
    "    model.inputfooling_OFF()\n",
    "    fooled = 0\n",
    "    for INDEX in trange(N):\n",
    "        success, x_adv, grad_adv = GradCertModule.run_tabular_attack(model, torch.Tensor(X_test[INDEX]), iterations=100,\n",
    "                                                      target=-1, epsilon=EPS, lr=0.05)\n",
    "        fooled += float(success)\n",
    "        #print(success)\n",
    "    model.inputfooling_OFF()\n",
    "    print(fooled/N)\n",
    "    return fooled/N\n",
    "    #print(\"Input Attack Fooling Rate: \", fooled/5)\n",
    "    \n",
    "    \n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "std_utarg_inp_attk = get_untarg_attk(MODEL_ID, EPS=0.05, N=30)\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "rob_utarg_inp_attk = get_untarg_attk(M_ID, EPS=0.05, N=30)\n",
    "\n",
    "M_ID = \"ADULT_L2_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "el2_utarg_inp_attk = get_untarg_attk(M_ID, EPS=0.05, N=30)\n",
    "\n",
    "M_ID = \"ADULT_HESSIAN_FCN_e=0.05_g=0.0_a=0.1_l=2_h=256_s=True\"\n",
    "hes_utarg_inp_attk = get_untarg_attk(M_ID, EPS=0.05, N=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untargeted Model Attack \n",
    "\n",
    "# Measure Model Attack Robustness\n",
    "def get_model_untarg(MODEL_ID, GAM=0.5, N=10):\n",
    "    fooled = 0\n",
    "    for INDEX in trange(N):\n",
    "        model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n",
    "        model.inputfooling_ON()\n",
    "        #success, x_adv, grad_adv = GradCertModule.run_tabular_model_attack(model, torch.Tensor(X_test[INDEX]), iterations=150,\n",
    "        #                                              target=sens_inds, gamma=GAM, lr=0.005*GAM, idx=7) \n",
    "        success, x_adv, grad_adv = GradCertModule.run_tabular_model_attack(model, torch.Tensor(X_test[INDEX]), iterations=50,\n",
    "                                                      target=-1, gamma=GAM, lr=0.02, idx=7) \n",
    "        \n",
    "        \n",
    "        #print(grad_adv)\n",
    "        fooled += float(success)\n",
    "    model.inputfooling_OFF()  \n",
    "    print(fooled/N)\n",
    "    return fooled/N\n",
    "    #print(\"Model Attack Fooling Rate: \", fooled/5)\n",
    "\n",
    "\n",
    "\n",
    "#get_model_attk(MODEL_ID, GAM=0.01, N=10)\n",
    "\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "std_utarg_mod_attk = get_model_untarg(M_ID, GAM=0.2, N=30)\n",
    "\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.05_a=1.0_l=2_h=256_s=True\"\n",
    "rob_utarg_mod_attk = get_model_untarg(M_ID, GAM=0.25, N=30)\n",
    "\n",
    "M_ID = \"ADULT_L2_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "el2_utarg_mod_attk = get_model_untarg(M_ID, GAM=0.25, N=30)\n",
    "\n",
    "M_ID = \"ADULT_HESSIAN_FCN_e=0.05_g=0.0_a=0.1_l=2_h=256_s=True\"\n",
    "hes_utarg_mod_attk = get_model_untarg(M_ID, GAM=0.25, N=30)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92fc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_cert_untarg(MODEL_ID, EPS=0.2):\n",
    "    model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n",
    "    import copy\n",
    "    certified = 0\n",
    "    for INDEX in trange(200):\n",
    "        lower, upper = GradCertModule.GradCertBounds(model, torch.Tensor(X_test[INDEX][None, :]),\n",
    "                                                     y_test[INDEX], eps=EPS, gam=0.00, nclasses=2)\n",
    "\n",
    "        upper = np.squeeze(upper.detach().numpy())\n",
    "        lower = np.squeeze(lower.detach().numpy())\n",
    "        diff = np.mean(upper-lower)\n",
    "        certified += float(diff)\n",
    "        #break\n",
    "    #print(\"Input Attack Certified: \", certified/200)\n",
    "    print(certified/200)\n",
    "    return certified/200\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "std_utarg_inp_cert = get_input_cert_untarg(MODEL_ID, EPS=0.05)\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "rob_utarg_inp_cert = get_input_cert_untarg(M_ID, EPS=0.05)\n",
    "\n",
    "M_ID = \"ADULT_L2_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "el2_utarg_inp_cert = get_input_cert_untarg(M_ID, EPS=0.05)\n",
    "\n",
    "M_ID = \"ADULT_HESSIAN_FCN_e=0.05_g=0.0_a=0.1_l=2_h=256_s=True\"\n",
    "hes_utarg_inp_cert = get_input_cert_untarg(M_ID, EPS=0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec78408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_cert(MODEL_ID, GAM=0.2):\n",
    "    model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n",
    "    import copy\n",
    "    certified = 0\n",
    "    for INDEX in trange(200):\n",
    "        lower, upper = GradCertModule.GradCertBounds(model, torch.Tensor(X_test[INDEX][None, :]),\n",
    "                                                     y_test[INDEX], eps=0.00, gam=GAM, nclasses=2)\n",
    "\n",
    "        upper = np.squeeze(upper.detach().numpy())\n",
    "        lower = np.squeeze(lower.detach().numpy())\n",
    "        diff = np.mean(upper-lower)\n",
    "        certified += float(diff)\n",
    "        #break\n",
    "    print(certified/200)\n",
    "    return certified/200\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "std_utarg_mod_cert = get_model_cert(MODEL_ID, GAM=0.03)\n",
    "\n",
    "M_ID = \"ADULT_FCN_e=0.0_g=0.05_a=1.0_l=2_h=256_s=True\"\n",
    "rob_utarg_mod_cert = get_model_cert(M_ID, GAM=0.03)\n",
    "\n",
    "M_ID = \"ADULT_L2_FCN_e=0.05_g=0.0_a=1.0_l=2_h=256_s=True\"\n",
    "el2_utarg_mod_cert = get_model_cert(M_ID, GAM=0.03)\n",
    "\n",
    "M_ID = \"ADULT_HESSIAN_FCN_e=0.05_g=0.0_a=0.1_l=2_h=256_s=True\"\n",
    "hes_utarg_mod_cert = get_model_cert(M_ID, GAM=0.03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "\n",
    "colors = sns.color_palette(\"Blues\", 4)\n",
    "\n",
    "std_vals = np.asarray([std_acc, 1-std_targ_inp_attk, 1-std_targ_mod_attk, \n",
    "            std_targ_inp_cert, std_targ_mod_cert])\n",
    "std_vals = np.clip(std_vals, 0, 1)\n",
    "std_vals = np.round(std_vals,2)\n",
    "\n",
    "rob_vals = np.asarray([rob_acc, 1-rob_targ_inp_attk, 1-rob_targ_mod_attk, \n",
    "            rob_targ_inp_cert, rob_targ_mod_cert])\n",
    "rob_vals = np.clip(rob_vals, 0, 1)\n",
    "rob_vals = np.round(rob_vals,2)\n",
    "\n",
    "el2_vals = np.asarray([el2_acc, 1-el2_targ_inp_attk, 1-el2_targ_mod_attk, \n",
    "            el2_targ_inp_cert, el2_targ_mod_cert])\n",
    "el2_vals = np.clip(el2_vals, 0, 1)\n",
    "el2_vals = np.round(el2_vals,2)\n",
    "\n",
    "hes_vals = np.asarray([hes_acc, 1-hes_targ_inp_attk, 1-hes_targ_mod_attk, \n",
    "            hes_targ_inp_cert, hes_targ_mod_cert])\n",
    "hes_vals = np.clip(hes_vals, 0, 1)\n",
    "hes_vals = np.round(hes_vals,2)\n",
    "\n",
    "x = np.arange(len(std_vals))*1.0  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,7), dpi=100)\n",
    "rects1 = ax.bar(x - width*2, std_vals, width, label='Standard', color=colors[0], )\n",
    "rects2 = ax.bar(x - width, rob_vals, width, label='Grad Cert.', color=colors[1])\n",
    "rects3 = ax.bar(x + 0, el2_vals, width, label='L2 Reg.', color=colors[2])\n",
    "rects4 = ax.bar(x + width, hes_vals, width, label='Hess. Reg.', color=colors[3])\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "labels = [\"(Accuracy)\", \"(Inp. Attk. Rob.)\", \"(Mod. Attk. Rob.)\", \"(Inp. Cert. Rob.)\", \"(Mod. Cert. Rob.)\"]\n",
    "ax.set_xticks(x, labels, size=20)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=40, label_type='center', rotation = 90, size=22)\n",
    "ax.bar_label(rects2, padding=40, label_type='center',  rotation = 90, size=22)\n",
    "ax.bar_label(rects3, padding=40, label_type='center', rotation = 90, size=22)\n",
    "ax.bar_label(rects4, padding=40, label_type='center',  rotation = 90, size=22)\n",
    "\n",
    "plt.ylim((0,1.05))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.ylabel('Robustness (/Accuracy)')\n",
    "plt.title(\"Adult - Targeted Attacks on Explanations (Higher is Better)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "\n",
    "colors = sns.color_palette(\"Blues\", 4)\n",
    "\n",
    "std_vals = np.asarray([ std_utarg_inp_attk, std_utarg_mod_attk, \n",
    "            std_utarg_inp_cert, std_utarg_mod_cert])\n",
    "std_vals = np.clip(std_vals, 0, 1)\n",
    "std_vals = np.round(std_vals,3)\n",
    "\n",
    "rob_vals = np.asarray([rob_utarg_inp_attk, rob_utarg_mod_attk, \n",
    "            rob_utarg_inp_cert, rob_utarg_mod_cert])\n",
    "rob_vals = np.clip(rob_vals, 0, 1)\n",
    "rob_vals = np.round(rob_vals,3)\n",
    "\n",
    "el2_vals = np.asarray([el2_utarg_inp_attk, el2_utarg_mod_attk, \n",
    "            el2_utarg_inp_cert, el2_utarg_mod_cert])\n",
    "el2_vals = np.clip(el2_vals, 0, 1)\n",
    "el2_vals = np.round(el2_vals,3)\n",
    "\n",
    "hes_vals = np.asarray([hes_utarg_inp_attk, hes_utarg_mod_attk, \n",
    "            hes_utarg_inp_cert, hes_utarg_mod_cert])\n",
    "hes_vals = np.clip(hes_vals, 0, 1)\n",
    "hes_vals = np.round(hes_vals,3)\n",
    "\n",
    "x = np.arange(len(std_vals))*1.5  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,7), dpi=200)\n",
    "rects1 = ax.bar(x - width*2, std_vals, width, label='Standard', color=colors[0], )\n",
    "rects2 = ax.bar(x - width, rob_vals, width, label='Grad Cert.', color=colors[1])\n",
    "rects3 = ax.bar(x + 0, el2_vals, width, label='L2 Reg.', color=colors[2])\n",
    "rects4 = ax.bar(x + width, hes_vals, width, label='Hess. Reg.', color=colors[3])\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "labels = [\"(Inp. Attk. Rob.)\", \"(Mod. Attk. Rob.)\", \"(Inp. Cert. Rob.)\", \"(Mod. Cert. Rob.)\"]\n",
    "ax.set_xticks(x, labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=30, label_type='center', rotation = 90, size=22)\n",
    "ax.bar_label(rects2, padding=30, label_type='center',  rotation = 90, size=22)\n",
    "ax.bar_label(rects3, padding=30, label_type='center', rotation = 90, size=22)\n",
    "ax.bar_label(rects4, padding=30, label_type='center',  rotation = 90, size=22)\n",
    "\n",
    "plt.title(\"Adult - Untargetted Explanation Attacks (Lower is Better)\")\n",
    "plt.ylabel(r\"Value of $\\delta$\")\n",
    "plt.ylim((0,1.05))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd5f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918adab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757706a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Input Attack Certification\n",
    "def get_model_cert(MODEL_ID, GAM=0.2):\n",
    "    model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n",
    "    import copy\n",
    "    certified = 0\n",
    "    for INDEX in trange(200):\n",
    "        lower, upper = GradCertModule.GradCertBounds(model, torch.Tensor(X_test[INDEX][None, :]),\n",
    "                                                     y_test[INDEX], eps=0.00, gam=GAM, nclasses=2)\n",
    "\n",
    "        upper = np.squeeze(upper.detach().numpy())\n",
    "        lower = np.squeeze(lower.detach().numpy())\n",
    "        #print(upper[sens_inds])\n",
    "        #print(lower[sens_inds])\n",
    "        temp = copy.deepcopy(lower)\n",
    "        for i in sens_inds:\n",
    "            temp[i] = upper[i]\n",
    "        #print(temp[sens_inds])\n",
    "        top_idx = np.squeeze(np.argsort(temp))\n",
    "        top_idx = list(reversed(top_idx))\n",
    "        #print(set(top_idx[0:10]))\n",
    "        #print( set(sens_inds))\n",
    "        cert = not bool(set(top_idx[0:5]) & set(sens_inds))\n",
    "        certified += int(cert)\n",
    "        #break\n",
    "    return certified/200\n",
    "    #print(\"Model Attack Certified: \", certified/200)\n",
    "get_model_cert(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55380423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Joint Attack Certification\n",
    "model.load_state_dict(torch.load('Models/%s.pt'%(MODEL_ID)))\n",
    "import copy\n",
    "certified = 0\n",
    "for INDEX in trange(200):\n",
    "    lower, upper = GradCertModule.GradCertBounds(model, torch.Tensor(X_test[INDEX][None, :]),\n",
    "                                                 y_test[INDEX], eps=0.2, gam=0.2, nclasses=2)\n",
    "    \n",
    "    upper = np.squeeze(upper.detach().numpy())\n",
    "    lower = np.squeeze(lower.detach().numpy())\n",
    "    #print(upper[sens_inds])\n",
    "    #print(lower[sens_inds])\n",
    "    temp = copy.deepcopy(lower)\n",
    "    for i in sens_inds:\n",
    "        temp[i] = upper[i]\n",
    "    #print(temp[sens_inds])\n",
    "    top_idx = np.squeeze(np.argsort(temp))\n",
    "    top_idx = list(reversed(top_idx))\n",
    "    #print(set(top_idx[0:10]))\n",
    "    #print( set(sens_inds))\n",
    "    cert = not bool(set(top_idx[0:5]) & set(sens_inds))\n",
    "    certified += int(cert)\n",
    "    #break\n",
    "print(\"Joint Attack Certified: \", certified/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Certified Adversarial Robustness\n",
    "\n",
    "robust = 0\n",
    "for INDEX in trange(len(X_test)):\n",
    "    data = torch.Tensor([X_test[INDEX]])\n",
    "    out, cls = model.classify(data)\n",
    "    y_l, y_u = GradCertModule.RobustnessBounds(model, data, cls, eps=0.1, gam=0.0, nclasses=2)\n",
    "    y_l, y_u = np.squeeze(y_l.detach().numpy()), np.squeeze(y_u.detach().numpy())\n",
    "    y = y_u\n",
    "    y[cls] = y_l[cls]\n",
    "    worst_case = np.argmax(y)\n",
    "    if(cls == y_test[INDEX]):\n",
    "        robust += 1 \n",
    "robust /= len(X_test)\n",
    "print(\"Certified Adversarial Robustness: \", robust)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model_id(GAM_T=0.0, EPS_T=0.0):\n",
    "\n",
    "    ALPHA = 1.0            # Regularization Parameter (Weights the Reg. Term)\n",
    "    EPSILON = EPS_T         # Input Peturbation Budget at Training Time\n",
    "\n",
    "    LEARN_RATE = 0.0005     # Learning Rate Hyperparameter\n",
    "    HIDDEN_DIM = 256       # Hidden Neurons Hyperparameter\n",
    "    HIDDEN_LAY = 2         # Hidden Layers Hyperparameter\n",
    "    MAX_EPOCHS = 25\n",
    "\n",
    "    EPSILON_LINEAR = True   # Put Epsilon on a Linear Schedule?\n",
    "    GAMMA_LINEAR = True     # Put Gamma on a Linear Schedule?\n",
    "    \n",
    "    MODEL_ID = \"%s_FCN_e=%s_g=%s_a=%s_l=%s_h=%s_s=%s\"%(dataset, EPSILON, GAM_T, ALPHA, HIDDEN_LAY, HIDDEN_DIM, SCHEDULED)     \n",
    "    print(MODEL_ID)\n",
    "    return MODEL_ID\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3bc668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CERT_VALS = []\n",
    "eps_vals = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "for eps in eps_vals:\n",
    "    M_ID = gen_model_id(EPS_T=eps)\n",
    "    val = []\n",
    "    for e_test in np.linspace(0, 0.2, 20): #[0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]:\n",
    "        certified = get_input_cert(M_ID, EPS=e_test)\n",
    "        val.append(certified)\n",
    "    #print(\"*****\")\n",
    "    #print(gam, val)\n",
    "    #print(\"*****\")\n",
    "    CERT_VALS.append(val)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cc26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.set_style('darkgrid')\n",
    "sns.set_context('poster')\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "\n",
    "\n",
    "eps_vals = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "g_test = [0.0, 0.01, 0.02, 0.03, 0.05, 0.06]\n",
    "\n",
    "pal = sns.cubehelix_palette(n_colors=len(eps_vals), start=.5, rot=-.75)\n",
    "print(pal.as_hex())\n",
    "\n",
    "for i in range(len(eps_vals)):\n",
    "    print(CERT_VALS[i])\n",
    "    plt.plot(CERT_VALS[i], label=eps_vals[i], \n",
    "             color=pal[i], linewidth=10)\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([0,4,9,14,19])\n",
    "labs = [round(i, 2) for i in np.linspace(0, 0.2, 5)]\n",
    "ax.set_xticklabels(labs)\n",
    "#ax.set_xticklabels(g_test)\n",
    "plt.title(\"%s\"%(dataset))\n",
    "plt.ylabel(\"Input Certified Robustness\")\n",
    "plt.xlabel(r\"Magnitude of $\\epsilon$\")\n",
    "ax.get_legend().set_title(r\"$\\epsilon_t$\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CERT_VALS = []\n",
    "if(dataset == \"ADULT\"):\n",
    "    gam_vals = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "    #gamma_vals = [0.0, 0.01, 0.02, 0.03, 0.05]\n",
    "#else:\n",
    "#    gam_vals = [0.0, 0.0125, 0.025, 0.05, 0.075, 0.125]\n",
    "for gam in gam_vals:\n",
    "    M_ID = gen_model_id(gam)\n",
    "    val = []\n",
    "    for g_test in np.linspace(0, 0.1, 20): #[0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]:\n",
    "        certified = get_model_cert(M_ID, GAM=g_test)\n",
    "        val.append(certified)\n",
    "    print(\"*****\")\n",
    "    print(gam, val)\n",
    "    print(\"*****\")\n",
    "    CERT_VALS.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29091e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.set_style('darkgrid')\n",
    "sns.set_context('poster')\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "\n",
    "# These values are different because of the logit softmax\n",
    "# comutation that we used to do vs what we do now given \n",
    "# That it was an error before. Just correct this when running plots\n",
    "\n",
    "if(dataset == \"ADULT\"):\n",
    "    gamma_vals = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "    g_test = [0.0, 0.01, 0.02, 0.03, 0.05, 0.06]\n",
    "#else:\n",
    "#    g_test = [0.0, 0.05, 0.1, 0.15, 0.2]\n",
    "#    gamma_vals = [0.0, 0.0125, 0.025, 0.05, 0.075, 0.125]\n",
    "pal = sns.cubehelix_palette(n_colors=len(gamma_vals), start=.5, rot=-.75)\n",
    "print(pal.as_hex())\n",
    "\n",
    "for i in range(len(gam_vals)):\n",
    "    print(CERT_VALS[i])\n",
    "    plt.plot(CERT_VALS[i], label=gamma_vals[i], \n",
    "             color=pal[i], linewidth=10)\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([0,4,9,14,19])\n",
    "labs = [round(i, 2) for i in np.linspace(0, 0.1, 5)]\n",
    "ax.set_xticklabels(labs)\n",
    "plt.title(\"%s\"%(dataset))\n",
    "plt.ylabel(\"Model Certified Robustness\")\n",
    "plt.xlabel(r\"Magnitude of $\\gamma$\")\n",
    "ax.get_legend().set_title(r\"$\\gamma_t$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTK_VALS = []\n",
    "\n",
    "eps_vals =  [0.00, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "for eps in eps_vals:\n",
    "    M_ID = gen_model_id(0.0, EPS_T = eps)\n",
    "    val = []\n",
    "    for e_test in [0.0, 0.05, 0.1, 0.15, 0.20]:\n",
    "        certified = get_input_attk(M_ID, EPS=e_test, N=35)\n",
    "        print(certified)\n",
    "        val.append(certified)\n",
    "    print(\"*****\")\n",
    "    print(eps, val)\n",
    "    print(\"*****\")\n",
    "    ATTK_VALS.append(val)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e42266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.set_style('darkgrid')\n",
    "sns.set_context('poster')\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "\n",
    "\n",
    "eps_vals = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "e_test_vals = [0.01, 0.05, 0.075, 0.1, 0.125, 0.175]\n",
    "\n",
    "pal = sns.cubehelix_palette()\n",
    "pal = pal.as_hex()\n",
    "pal = [i for i in reversed(pal)]\n",
    "print(pal)\n",
    "#print(pal.as_hex())\n",
    "\n",
    "for i in range(len(eps_vals)):\n",
    "    plt.plot(1-np.asarray(ATTK_VALS[i]), label=eps_vals[i], color=pal[i], linewidth=10)\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_xticks(range(len(e_test_vals)))\n",
    "ax.set_xticklabels(e_test_vals)\n",
    "\n",
    "plt.title(\"%s\"%(dataset))\n",
    "plt.ylabel(\"Input Attack Robustness\")\n",
    "plt.xlabel(r\"Magnitude of $\\epsilon$\")\n",
    "ax.get_legend().set_title(r\"$\\epsilon_t$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734008b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ATTK_VALS = []\n",
    "\n",
    "gam_vals = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "for gam in gam_vals:\n",
    "    M_ID = gen_model_id(gam)\n",
    "    val = []\n",
    "    for g_test in [0.01, 0.05, 0.075, 0.1, 0.125, 0.175]:\n",
    "        certified = get_model_attk(M_ID, GAM=g_test, N=35)\n",
    "        print(certified)\n",
    "        val.append(certified)\n",
    "    print(\"*****\")\n",
    "    print(gam, val)\n",
    "    print(\"*****\")\n",
    "    ATTK_VALS.append(val)\n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e86209",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ATTK_VALS)\n",
    "\n",
    "ATTK_VALS = [[0.2, 0.5714285714285714, 0.6, 0.6571428571428571, 0.7142857142857143, 0.8],\n",
    "             [0.0, 0.0, 0.0, 0.0, 0.17142857142857143, 0.4], \n",
    "             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
    "             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
    "             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.set_style('darkgrid')\n",
    "sns.set_context('poster')\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "\n",
    "\n",
    "gam_vals = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "g_test_vals = [0.05, 0.075, 0.1, 0.125, 0.15]\n",
    "\n",
    "pal = sns.cubehelix_palette()\n",
    "pal = pal.as_hex()\n",
    "pal = [i for i in reversed(pal)]\n",
    "print(pal)\n",
    "#print(pal.as_hex())\n",
    "\n",
    "for i in range(len(gam_vals)):\n",
    "    plt.plot(1-np.asarray(ATTK_VALS[i]), label=gam_vals[i], color=pal[i], linewidth=10)\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_xticks(range(len(g_test_vals)))\n",
    "ax.set_xticklabels(g_test_vals)\n",
    "\n",
    "plt.title(\"%s\"%(dataset))\n",
    "plt.ylabel(\"Model Attack Robustness\")\n",
    "plt.xlabel(r\"Magnitude of $\\gamma$\")\n",
    "ax.get_legend().set_title(r\"$\\gamma_t$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcaabcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ATTK_VALS)\n",
    "ATTK_VALS = [[0.2, 0.5714285714285714, 0.6, 0.6571428571428571, 0.7142857142857143, 0.8],\n",
    "             [0.0, 0.0, 0.0, 0.0, 0.17142857142857143, 0.4], \n",
    "             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
    "             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
    "             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
