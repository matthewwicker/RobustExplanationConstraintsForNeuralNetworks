{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2159e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# *** I need to actually do something more sophisicated with normalization to ensure that these\n",
    "# inputs are normalized in the same waya as the ones we trained on, but Torch seems\n",
    "# a bit complicated to me on this front so I am ignoring it for now :) \n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "mnist_trainset = datasets.MNIST(root='./Datasets', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./Datasets', train=False, download=True, transform=None)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import XAIArchitectures\n",
    "import GradCertModule\n",
    "import pytorch_lightning as pl\n",
    "from pl_examples.basic_examples.mnist_datamodule import MNISTDataModule\n",
    "\n",
    "\n",
    "ALPHA = 0.0            # Regularization Parameter (Weights the Reg. Term)\n",
    "EPSILON = 0.0          # Input Peturbation Budget at Training Time\n",
    "GAMMA = 0.0             # Model Peturbation Budget at Training Time \n",
    "                        #(Changed to proportional budget rather than absolute)\n",
    "    \n",
    "LEARN_RATE = 0.001      # Learning Rate Hyperparameter\n",
    "HIDDEN_DIM = 100       # Hidden Neurons Hyperparameter\n",
    "HIDDEN_LAY = 1         # Hidden Layers Hyperparameter\n",
    "MAX_EPOCHS = 10\n",
    "\n",
    "EPSILON_LINEAR = True   # Put Epsilon on a Linear Schedule?\n",
    "GAMMA_LINEAR = True     # Put Gamma on a Linear Schedule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9673d8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\n",
      "global_step\n",
      "pytorch-lightning_version\n",
      "state_dict\n",
      "callbacks\n",
      "optimizer_states\n",
      "lr_schedulers\n",
      "hparams_name\n",
      "hyper_parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XAIArchitectures.DeepMindSmall()\n",
    "\n",
    "SCHEDULED = EPSILON_LINEAR or GAMMA_LINEAR \n",
    "#DeepMindSmall_e=0.0_g=0.0_a=0.0_s=True.ckpt\n",
    "MODEL_ID = \"DeepMindSmall_e=%s_g=%s_a=%s_s=%s\"%(EPSILON, GAMMA, ALPHA, SCHEDULED)\n",
    "#MODEL_ID = \"DeepMindSmall_e=%s_g=%s_h=%s_l=%s_a=%s_s=%s\"%(EPSILON, GAMMA, HIDDEN_DIM, HIDDEN_LAY, ALPHA, SCHEDULED)   \n",
    "ckpt = torch.load(\"GPU_Models/%s.ckpt\"%(MODEL_ID))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "checkpoint = torch.load(\"GPU_Models/%s.ckpt\"%(MODEL_ID))\n",
    "for key in checkpoint:\n",
    "    print(key)\n",
    "model.load_state_dict(torch.load('GPU_Models/%s.pt'%(MODEL_ID)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48033598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewwicker/opt/anaconda3/envs/XAIenvironment/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984\n"
     ]
    }
   ],
   "source": [
    "print(mnist_testset.data.shape)\n",
    "\n",
    "acc = 0\n",
    "for i in range(1000):\n",
    "    data = np.asarray(mnist_testset[i][0]).reshape(1,28,28)/255.0\n",
    "    target = torch.Tensor([mnist_testset[i][1]]).type(torch.LongTensor)\n",
    "    #model(torch.Tensor(data[None, :]))\n",
    "    out = torch.argmax(model(torch.Tensor([data])))\n",
    "    if(target == out):\n",
    "        acc += 1\n",
    "print(acc/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071bb681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewwicker/opt/anaconda3/envs/XAIenvironment/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: `multichannel` is a deprecated argument name for `gaussian`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  if sys.path[0] == \"\":\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import skimage\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "INDEX = 0\n",
    "targex = np.asarray(mnist_testset[INDEX][0]).reshape(28,28) * 0.0\n",
    "targex[8,8] = 10\n",
    "sigma = 3.0\n",
    "targex = skimage.filters.gaussian(\n",
    "    targex, sigma=(sigma, sigma), truncate=3.5, multichannel=True)\n",
    "\n",
    "targex = (targex-np.min(targex))/(np.max(targex)-np.min(targex))\n",
    "\n",
    "plt.imshow(targex.reshape(28,28), cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "targex = torch.Tensor(targex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b742c770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NEW TARGET\n",
    "\"\"\"\n",
    "INDEX = 0\n",
    "targex = np.asarray(mnist_testset[INDEX][0]).reshape(28,28) * 0.0\n",
    "targex += 1\n",
    "targex[5:23,5:23] = 0\n",
    "#sigma = 9.0\n",
    "#targex = skimage.filters.gaussian(\n",
    "#    targex, sigma=(sigma, sigma), truncate=3.5, multichannel=True)\n",
    "\n",
    "#targex = (targex-np.min(targex))/(np.max(targex)-np.min(targex))\n",
    "#targex = 1-targex\n",
    "plt.imshow(targex.reshape(28,28), cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "targex = torch.Tensor(targex)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1158c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b1ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_magic(model, x, desired_index):\n",
    "    # calculate the integrand in one batch\n",
    "    # we use DataParallel mode of model to fit the batch in memory of (multiple) gpu(s)\n",
    "    num_summands = 30\n",
    "    prefactors = x.new_tensor([k / num_summands for k in range(1, num_summands + 1)])\n",
    "    parallel_model = torch.nn.DataParallel(model)\n",
    "    y = parallel_model(prefactors.view(num_summands, 1, 1, 1) * x)\n",
    "\n",
    "    # we sum the result and then take the derivative (instead of summing derivatives as in most implementations),\n",
    "    # (d/dx) (n*y_1(1/n*x) + n/2*y_1(2/n*x) .... + y_n(x) ) = y_1'+....y'_n\n",
    "    y = torch.nn.functional.softmax(y, 1)[:, int(desired_index)]\n",
    "    y = (1 / num_summands) * torch.sum(y / prefactors, dim=0)\n",
    "    heatmap = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7070994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nx = np.asarray(mnist_testset[INDEX][0]).reshape(1,28,28)/255.0\\nx = torch.Tensor(x)\\nx.requires_grad = True\\ny, cls = model.classify(torch.Tensor(x[None, :]))\\ngrad_adv = torch.abs(black_magic(model, x, cls))\\n\\nprint(torch.argmax(grad_adv))\\nprint(np.argmax(grad_adv.detach().numpy()))\\n# Top 5% indexes\\ninds = np.argpartition(grad_adv.detach().numpy().flatten(), -28)[-28:]\\ntargex = (grad_adv.flatten() * 0)+1 \\ntargex[inds] = 0\\ntargex = targex.reshape(28,28).detach().numpy()\\n\\nplt.imshow(targex, cmap='coolwarm')\\nplt.colorbar()\\nplt.show()\\n\\ntargex = torch.Tensor(targex)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x = np.asarray(mnist_testset[INDEX][0]).reshape(1,28,28)/255.0\n",
    "x = torch.Tensor(x)\n",
    "x.requires_grad = True\n",
    "y, cls = model.classify(torch.Tensor(x[None, :]))\n",
    "grad_adv = torch.abs(black_magic(model, x, cls))\n",
    "\n",
    "print(torch.argmax(grad_adv))\n",
    "print(np.argmax(grad_adv.detach().numpy()))\n",
    "# Top 5% indexes\n",
    "inds = np.argpartition(grad_adv.detach().numpy().flatten(), -28)[-28:]\n",
    "targex = (grad_adv.flatten() * 0)+1 \n",
    "targex[inds] = 0\n",
    "targex = targex.reshape(28,28).detach().numpy()\n",
    "\n",
    "plt.imshow(targex, cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "targex = torch.Tensor(targex)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca75dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewwicker/opt/anaconda3/envs/XAIenvironment/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Using a target size (torch.Size([28, 28])) that is different to the input size (torch.Size([1, 1, 28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Total Loss: 0.2918744683265686, Expl Loss: 0.5837489366531372, Output Loss: 1.4007516832917084e-14\n",
      "Iteration 1: Total Loss: 0.2918551564216614, Expl Loss: 0.5837103128433228, Output Loss: 7.853212437985979e-14\n",
      "Iteration 2: Total Loss: 0.29183563590049744, Expl Loss: 0.5836712718009949, Output Loss: 2.2699672545291316e-13\n",
      "Iteration 3: Total Loss: 0.2918158173561096, Expl Loss: 0.5836316347122192, Output Loss: 4.641255370481379e-13\n",
      "Iteration 4: Total Loss: 0.2917957007884979, Expl Loss: 0.5835914015769958, Output Loss: 8.008025666195184e-13\n",
      "Iteration 5: Total Loss: 0.29177477955818176, Expl Loss: 0.5835495591163635, Output Loss: 1.3117960493899683e-12\n",
      "Iteration 6: Total Loss: 0.2917533814907074, Expl Loss: 0.5835067629814148, Output Loss: 1.976304970369158e-12\n",
      "Iteration 7: Total Loss: 0.29173141717910767, Expl Loss: 0.5834628343582153, Output Loss: 2.812690184927913e-12\n",
      "Iteration 8: Total Loss: 0.2917090654373169, Expl Loss: 0.5834181308746338, Output Loss: 3.950315768941337e-12\n",
      "Iteration 9: Total Loss: 0.29168620705604553, Expl Loss: 0.5833724141120911, Output Loss: 5.469736642782497e-12\n",
      "Iteration 10: Total Loss: 0.29166263341903687, Expl Loss: 0.5833252668380737, Output Loss: 7.315612525482518e-12\n",
      "Iteration 11: Total Loss: 0.29163858294487, Expl Loss: 0.58327716588974, Output Loss: 9.700983072402636e-12\n",
      "Iteration 12: Total Loss: 0.29161399602890015, Expl Loss: 0.5832279920578003, Output Loss: 1.2556128012319867e-11\n",
      "Iteration 13: Total Loss: 0.29158875346183777, Expl Loss: 0.5831775069236755, Output Loss: 1.6395176882788576e-11\n",
      "Iteration 14: Total Loss: 0.29156294465065, Expl Loss: 0.5831258893013, Output Loss: 2.0975058495631416e-11\n",
      "Iteration 15: Total Loss: 0.2915365993976593, Expl Loss: 0.5830731987953186, Output Loss: 2.6984154494202883e-11\n",
      "Iteration 16: Total Loss: 0.2915095388889313, Expl Loss: 0.5830190777778625, Output Loss: 3.4465420101215116e-11\n",
      "Iteration 17: Total Loss: 0.2914818823337555, Expl Loss: 0.582963764667511, Output Loss: 4.374098305781615e-11\n",
      "Iteration 18: Total Loss: 0.2914535105228424, Expl Loss: 0.5829070210456848, Output Loss: 5.602437264107252e-11\n",
      "Iteration 19: Total Loss: 0.29142436385154724, Expl Loss: 0.5828487277030945, Output Loss: 7.120116995995573e-11\n",
      "Iteration 20: Total Loss: 0.29139444231987, Expl Loss: 0.58278888463974, Output Loss: 9.097005787550572e-11\n",
      "Iteration 21: Total Loss: 0.29136356711387634, Expl Loss: 0.5827271342277527, Output Loss: 1.1716042025433637e-10\n",
      "Iteration 22: Total Loss: 0.29133179783821106, Expl Loss: 0.5826635956764221, Output Loss: 1.5090675931084263e-10\n",
      "Iteration 23: Total Loss: 0.291299045085907, Expl Loss: 0.582598090171814, Output Loss: 1.950020839247557e-10\n",
      "Iteration 24: Total Loss: 0.2912651002407074, Expl Loss: 0.5825302004814148, Output Loss: 2.5410895609923045e-10\n",
      "Iteration 25: Total Loss: 0.291229784488678, Expl Loss: 0.582459568977356, Output Loss: 3.3279284883391824e-10\n",
      "Iteration 26: Total Loss: 0.2911929488182068, Expl Loss: 0.5823858976364136, Output Loss: 4.381897622529607e-10\n",
      "Iteration 27: Total Loss: 0.2911544442176819, Expl Loss: 0.5823088884353638, Output Loss: 5.838994288076549e-10\n",
      "Iteration 28: Total Loss: 0.2911137640476227, Expl Loss: 0.5822275280952454, Output Loss: 7.835144200107891e-10\n",
      "Iteration 29: Total Loss: 0.29107117652893066, Expl Loss: 0.5821423530578613, Output Loss: 1.0605412104069956e-09\n",
      "Iteration 30: Total Loss: 0.29102659225463867, Expl Loss: 0.5820531845092773, Output Loss: 1.4526337910325537e-09\n",
      "Iteration 31: Total Loss: 0.2909794747829437, Expl Loss: 0.5819589495658875, Output Loss: 2.007259469749556e-09\n",
      "Iteration 32: Total Loss: 0.2909295856952667, Expl Loss: 0.5818591713905334, Output Loss: 2.812481358915875e-09\n",
      "Iteration 33: Total Loss: 0.2908768355846405, Expl Loss: 0.581753671169281, Output Loss: 3.985902274905584e-09\n",
      "Iteration 34: Total Loss: 0.2908208668231964, Expl Loss: 0.5816417336463928, Output Loss: 5.722272877761725e-09\n",
      "Iteration 35: Total Loss: 0.2907610535621643, Expl Loss: 0.5815221071243286, Output Loss: 8.341115353971418e-09\n",
      "Iteration 36: Total Loss: 0.2906973958015442, Expl Loss: 0.5813947916030884, Output Loss: 1.2340154675882786e-08\n",
      "Iteration 37: Total Loss: 0.29062923789024353, Expl Loss: 0.5812584757804871, Output Loss: 1.85381630046777e-08\n",
      "Iteration 38: Total Loss: 0.29055580496788025, Expl Loss: 0.5811116099357605, Output Loss: 2.82938348306061e-08\n",
      "Iteration 39: Total Loss: 0.2904767394065857, Expl Loss: 0.5809534788131714, Output Loss: 4.389152152839415e-08\n",
      "Iteration 40: Total Loss: 0.29039114713668823, Expl Loss: 0.5807822942733765, Output Loss: 6.919515271874843e-08\n",
      "Iteration 41: Total Loss: 0.2902980446815491, Expl Loss: 0.5805960893630981, Output Loss: 1.1086356010991949e-07\n",
      "Iteration 42: Total Loss: 0.29019632935523987, Expl Loss: 0.5803926587104797, Output Loss: 1.8058214834582031e-07\n",
      "Iteration 43: Total Loss: 0.2900848090648651, Expl Loss: 0.5801695585250854, Output Loss: 2.9907022280895035e-07\n",
      "Iteration 44: Total Loss: 0.2899623215198517, Expl Loss: 0.5799245834350586, Output Loss: 5.036402512814675e-07\n",
      "Iteration 45: Total Loss: 0.28982678055763245, Expl Loss: 0.5796535015106201, Output Loss: 8.623043754596438e-07\n",
      "Iteration 46: Total Loss: 0.28967538475990295, Expl Loss: 0.5793505907058716, Output Loss: 1.5017764098956832e-06\n",
      "Iteration 47: Total Loss: 0.28950631618499756, Expl Loss: 0.579012393951416, Output Loss: 2.6594098017085344e-06\n",
      "Iteration 48: Total Loss: 0.28931713104248047, Expl Loss: 0.5786337852478027, Output Loss: 4.785971214005258e-06\n",
      "Iteration 49: Total Loss: 0.28910383582115173, Expl Loss: 0.5782067775726318, Output Loss: 8.743956641410477e-06\n",
      "Iteration 50: Total Loss: 0.2888616621494293, Expl Loss: 0.5777217149734497, Output Loss: 1.6195481293834746e-05\n",
      "Iteration 51: Total Loss: 0.28858980536460876, Expl Loss: 0.577176570892334, Output Loss: 3.034935798496008e-05\n",
      "Iteration 52: Total Loss: 0.28828585147857666, Expl Loss: 0.5765659809112549, Output Loss: 5.739468542742543e-05\n",
      "Iteration 53: Total Loss: 0.28794753551483154, Expl Loss: 0.5758841633796692, Output Loss: 0.00010922993533313274\n",
      "Iteration 54: Total Loss: 0.28757160902023315, Expl Loss: 0.5751223564147949, Output Loss: 0.0002086000022245571\n",
      "Iteration 55: Total Loss: 0.28715917468070984, Expl Loss: 0.5742784738540649, Output Loss: 0.0003985313815064728\n",
      "Iteration 56: Total Loss: 0.2867179214954376, Expl Loss: 0.5733599662780762, Output Loss: 0.0007587549043819308\n",
      "Iteration 57: Total Loss: 0.28626325726509094, Expl Loss: 0.5723832845687866, Output Loss: 0.0014320777263492346\n",
      "Iteration 58: Total Loss: 0.2858218550682068, Expl Loss: 0.5713778734207153, Output Loss: 0.002658168086782098\n",
      "Iteration 59: Total Loss: 0.2854357063770294, Expl Loss: 0.5703920722007751, Output Loss: 0.004793593194335699\n",
      "Iteration 60: Total Loss: 0.2851603627204895, Expl Loss: 0.5694973468780518, Output Loss: 0.008234080858528614\n",
      "Iteration 61: Total Loss: 0.2850327491760254, Expl Loss: 0.5687565803527832, Output Loss: 0.013089140877127647\n",
      "Iteration 62: Total Loss: 0.2850274443626404, Expl Loss: 0.568187415599823, Output Loss: 0.018674662336707115\n",
      "Iteration 63: Total Loss: 0.28504857420921326, Expl Loss: 0.5677434802055359, Output Loss: 0.02353677712380886\n",
      "Iteration 64: Total Loss: 0.28499481081962585, Expl Loss: 0.567348062992096, Output Loss: 0.026415402069687843\n",
      "Iteration 65: Total Loss: 0.2848242521286011, Expl Loss: 0.5669570565223694, Output Loss: 0.026914212852716446\n",
      "Iteration 66: Total Loss: 0.2845585346221924, Expl Loss: 0.5665793418884277, Output Loss: 0.025377046316862106\n",
      "Iteration 67: Total Loss: 0.2842518389225006, Expl Loss: 0.5662526488304138, Output Loss: 0.022510269656777382\n",
      "Iteration 68: Total Loss: 0.28396183252334595, Expl Loss: 0.5660144090652466, Output Loss: 0.019092531874775887\n",
      "Iteration 69: Total Loss: 0.28372907638549805, Expl Loss: 0.5658807754516602, Output Loss: 0.01577363908290863\n",
      "Iteration 70: Total Loss: 0.2835649847984314, Expl Loss: 0.5658326148986816, Output Loss: 0.012973596341907978\n",
      "Iteration 71: Total Loss: 0.28345656394958496, Expl Loss: 0.5658270120620728, Output Loss: 0.010860929265618324\n",
      "Iteration 72: Total Loss: 0.2833767235279083, Expl Loss: 0.5658101439476013, Output Loss: 0.009433025494217873\n",
      "Iteration 73: Total Loss: 0.28329721093177795, Expl Loss: 0.5657328963279724, Output Loss: 0.008615021593868732\n",
      "Iteration 74: Total Loss: 0.28319695591926575, Expl Loss: 0.5655609965324402, Output Loss: 0.008328927680850029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75: Total Loss: 0.28306567668914795, Expl Loss: 0.5652784705162048, Output Loss: 0.008528714068233967\n",
      "Iteration 76: Total Loss: 0.28290265798568726, Expl Loss: 0.5648844838142395, Output Loss: 0.009208502247929573\n",
      "Iteration 77: Total Loss: 0.28271573781967163, Expl Loss: 0.5643918514251709, Output Loss: 0.010396435856819153\n",
      "Iteration 78: Total Loss: 0.28252047300338745, Expl Loss: 0.5638278126716614, Output Loss: 0.012131156399846077\n",
      "Iteration 79: Total Loss: 0.2823360860347748, Expl Loss: 0.5632311701774597, Output Loss: 0.014410203322768211\n",
      "Iteration 80: Total Loss: 0.282179594039917, Expl Loss: 0.562646210193634, Output Loss: 0.01712993159890175\n",
      "Iteration 81: Total Loss: 0.28205636143684387, Expl Loss: 0.5621107220649719, Output Loss: 0.02002016082406044\n",
      "Iteration 82: Total Loss: 0.28195419907569885, Expl Loss: 0.5616446137428284, Output Loss: 0.022637687623500824\n",
      "Iteration 83: Total Loss: 0.281850665807724, Expl Loss: 0.5612526535987854, Output Loss: 0.024486487731337547\n",
      "Iteration 84: Total Loss: 0.2817249000072479, Expl Loss: 0.5609284043312073, Output Loss: 0.025213807821273804\n",
      "Iteration 85: Total Loss: 0.2815719246864319, Expl Loss: 0.5606656670570374, Output Loss: 0.024782059714198112\n",
      "Iteration 86: Total Loss: 0.2814028263092041, Expl Loss: 0.5604642033576965, Output Loss: 0.023414602503180504\n",
      "Iteration 87: Total Loss: 0.2812373638153076, Expl Loss: 0.5603208541870117, Output Loss: 0.02153870463371277\n",
      "Iteration 88: Total Loss: 0.28108856081962585, Expl Loss: 0.5602182149887085, Output Loss: 0.019588997587561607\n",
      "Iteration 89: Total Loss: 0.28095823526382446, Expl Loss: 0.560126543045044, Output Loss: 0.017899202182888985\n",
      "Iteration 90: Total Loss: 0.2808380424976349, Expl Loss: 0.5600088834762573, Output Loss: 0.01667202077805996\n",
      "Iteration 91: Total Loss: 0.2807169556617737, Expl Loss: 0.5598346590995789, Output Loss: 0.015992265194654465\n",
      "Iteration 92: Total Loss: 0.2805871367454529, Expl Loss: 0.5595872402191162, Output Loss: 0.015870196744799614\n",
      "Iteration 93: Total Loss: 0.2804454565048218, Expl Loss: 0.5592613816261292, Output Loss: 0.01629548706114292\n",
      "Iteration 94: Total Loss: 0.28029724955558777, Expl Loss: 0.5588764548301697, Output Loss: 0.01718045398592949\n",
      "Iteration 95: Total Loss: 0.2801482677459717, Expl Loss: 0.5584549307823181, Output Loss: 0.01841590739786625\n",
      "Iteration 96: Total Loss: 0.2800043225288391, Expl Loss: 0.5580259561538696, Output Loss: 0.01982703059911728\n",
      "Iteration 97: Total Loss: 0.2798672318458557, Expl Loss: 0.5576148629188538, Output Loss: 0.021195823326706886\n",
      "Iteration 98: Total Loss: 0.2797313928604126, Expl Loss: 0.5572425723075867, Output Loss: 0.02220212295651436\n",
      "Iteration 99: Total Loss: 0.27958980202674866, Expl Loss: 0.5569152235984802, Output Loss: 0.022643830627202988\n",
      "Iteration 100: Total Loss: 0.27943840622901917, Expl Loss: 0.556630551815033, Output Loss: 0.02246236242353916\n",
      "Iteration 101: Total Loss: 0.2792811989784241, Expl Loss: 0.5563861131668091, Output Loss: 0.02176293358206749\n",
      "Iteration 102: Total Loss: 0.2791255712509155, Expl Loss: 0.5561767816543579, Output Loss: 0.020743664354085922\n",
      "Iteration 103: Total Loss: 0.27897506952285767, Expl Loss: 0.5559816360473633, Output Loss: 0.019684789702296257\n",
      "Iteration 104: Total Loss: 0.2788299024105072, Expl Loss: 0.5557762384414673, Output Loss: 0.018835490569472313\n",
      "Iteration 105: Total Loss: 0.2786894142627716, Expl Loss: 0.5555448532104492, Output Loss: 0.018339861184358597\n",
      "Iteration 106: Total Loss: 0.2785486578941345, Expl Loss: 0.5552659630775452, Output Loss: 0.018313366919755936\n",
      "Iteration 107: Total Loss: 0.2784070670604706, Expl Loss: 0.554938793182373, Output Loss: 0.018753396347165108\n",
      "Iteration 108: Total Loss: 0.278267502784729, Expl Loss: 0.5545756816864014, Output Loss: 0.019593453034758568\n",
      "Iteration 109: Total Loss: 0.2781333923339844, Expl Loss: 0.5541967749595642, Output Loss: 0.020699823275208473\n",
      "Iteration 110: Total Loss: 0.27800723910331726, Expl Loss: 0.5538308024406433, Output Loss: 0.021836550906300545\n",
      "Iteration 111: Total Loss: 0.2778863310813904, Expl Loss: 0.5535001158714294, Output Loss: 0.022725606337189674\n",
      "Iteration 112: Total Loss: 0.2777650058269501, Expl Loss: 0.553214967250824, Output Loss: 0.023150626569986343\n",
      "Iteration 113: Total Loss: 0.277640700340271, Expl Loss: 0.5529769062995911, Output Loss: 0.023044755682349205\n",
      "Iteration 114: Total Loss: 0.2775139808654785, Expl Loss: 0.5527767539024353, Output Loss: 0.022511843591928482\n",
      "Iteration 115: Total Loss: 0.27738863229751587, Expl Loss: 0.5526002049446106, Output Loss: 0.0217706561088562\n",
      "Iteration 116: Total Loss: 0.2772662937641144, Expl Loss: 0.5524253249168396, Output Loss: 0.021072784438729286\n",
      "Iteration 117: Total Loss: 0.2771453261375427, Expl Loss: 0.5522304177284241, Output Loss: 0.02060210518538952\n",
      "Iteration 118: Total Loss: 0.2770228385925293, Expl Loss: 0.5519968271255493, Output Loss: 0.020488420501351357\n",
      "Iteration 119: Total Loss: 0.27689695358276367, Expl Loss: 0.5517129898071289, Output Loss: 0.020809471607208252\n",
      "Iteration 120: Total Loss: 0.2767687141895294, Expl Loss: 0.5513902306556702, Output Loss: 0.021471986547112465\n",
      "Iteration 121: Total Loss: 0.27664124965667725, Expl Loss: 0.5510543584823608, Output Loss: 0.022281300276517868\n",
      "Iteration 122: Total Loss: 0.2765151560306549, Expl Loss: 0.5507307052612305, Output Loss: 0.022996148094534874\n",
      "Iteration 123: Total Loss: 0.27638858556747437, Expl Loss: 0.5504361987113953, Output Loss: 0.02340957522392273\n",
      "Iteration 124: Total Loss: 0.2762601971626282, Expl Loss: 0.5501795411109924, Output Loss: 0.02340880036354065\n",
      "Iteration 125: Total Loss: 0.27613192796707153, Expl Loss: 0.5499613881111145, Output Loss: 0.023024920374155045\n",
      "Iteration 126: Total Loss: 0.2760048806667328, Expl Loss: 0.549763560295105, Output Loss: 0.022462060675024986\n",
      "Iteration 127: Total Loss: 0.27588194608688354, Expl Loss: 0.5495683550834656, Output Loss: 0.0219552144408226\n",
      "Iteration 128: Total Loss: 0.27576181292533875, Expl Loss: 0.5493563413619995, Output Loss: 0.021672967821359634\n",
      "Iteration 129: Total Loss: 0.27564364671707153, Expl Loss: 0.5491166114807129, Output Loss: 0.02170673944056034\n",
      "Iteration 130: Total Loss: 0.27552804350852966, Expl Loss: 0.5488556027412415, Output Loss: 0.022004704922437668\n",
      "Iteration 131: Total Loss: 0.2754155695438385, Expl Loss: 0.548586368560791, Output Loss: 0.022447552531957626\n",
      "Iteration 132: Total Loss: 0.27530595660209656, Expl Loss: 0.5483261942863464, Output Loss: 0.022857317700982094\n",
      "Iteration 133: Total Loss: 0.2751995921134949, Expl Loss: 0.5480920672416687, Output Loss: 0.02307094633579254\n",
      "Iteration 134: Total Loss: 0.2750953435897827, Expl Loss: 0.5478872060775757, Output Loss: 0.023034706711769104\n",
      "Iteration 135: Total Loss: 0.2749934792518616, Expl Loss: 0.5477088093757629, Output Loss: 0.02278144657611847\n",
      "Iteration 136: Total Loss: 0.27489444613456726, Expl Loss: 0.5475439429283142, Output Loss: 0.022449560463428497\n",
      "Iteration 137: Total Loss: 0.2747982144355774, Expl Loss: 0.5473758578300476, Output Loss: 0.022205453366041183\n",
      "Iteration 138: Total Loss: 0.2747035324573517, Expl Loss: 0.547190248966217, Output Loss: 0.022168047726154327\n",
      "Iteration 139: Total Loss: 0.27460983395576477, Expl Loss: 0.5469847917556763, Output Loss: 0.022349020466208458\n",
      "Iteration 140: Total Loss: 0.2745172083377838, Expl Loss: 0.546764075756073, Output Loss: 0.02270331233739853\n",
      "Iteration 141: Total Loss: 0.27442649006843567, Expl Loss: 0.546541690826416, Output Loss: 0.02311275713145733\n",
      "Iteration 142: Total Loss: 0.2743370532989502, Expl Loss: 0.5463321208953857, Output Loss: 0.023419653996825218\n",
      "Iteration 143: Total Loss: 0.27424874901771545, Expl Loss: 0.5461459755897522, Output Loss: 0.02351512387394905\n",
      "Iteration 144: Total Loss: 0.274161159992218, Expl Loss: 0.5459798574447632, Output Loss: 0.023424336686730385\n",
      "Iteration 145: Total Loss: 0.27407416701316833, Expl Loss: 0.5458232164382935, Output Loss: 0.02325134351849556\n",
      "Iteration 146: Total Loss: 0.27398788928985596, Expl Loss: 0.5456675887107849, Output Loss: 0.023082083091139793\n",
      "Iteration 147: Total Loss: 0.2739028036594391, Expl Loss: 0.5455039143562317, Output Loss: 0.023016953840851784\n",
      "Iteration 148: Total Loss: 0.2738182246685028, Expl Loss: 0.5453209280967712, Output Loss: 0.023155227303504944\n",
      "Iteration 149: Total Loss: 0.27373453974723816, Expl Loss: 0.5451230406761169, Output Loss: 0.023460501804947853\n",
      "Iteration 150: Total Loss: 0.27365151047706604, Expl Loss: 0.5449228286743164, Output Loss: 0.023801740258932114\n",
      "Iteration 151: Total Loss: 0.2735690772533417, Expl Loss: 0.5447329878807068, Output Loss: 0.02405187115073204\n",
      "Iteration 152: Total Loss: 0.27348679304122925, Expl Loss: 0.5445613265037537, Output Loss: 0.024122484028339386\n",
      "Iteration 153: Total Loss: 0.2734051048755646, Expl Loss: 0.544409990310669, Output Loss: 0.02400202676653862\n",
      "Iteration 154: Total Loss: 0.2733246088027954, Expl Loss: 0.5442665815353394, Output Loss: 0.02382608875632286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 155: Total Loss: 0.27324435114860535, Expl Loss: 0.5441204905509949, Output Loss: 0.02368193492293358\n",
      "Iteration 156: Total Loss: 0.2731657028198242, Expl Loss: 0.5439672470092773, Output Loss: 0.023641694337129593\n",
      "Iteration 157: Total Loss: 0.27308738231658936, Expl Loss: 0.5438005924224854, Output Loss: 0.02374148927628994\n",
      "Iteration 158: Total Loss: 0.2730097770690918, Expl Loss: 0.5436296463012695, Output Loss: 0.023899143561720848\n",
      "Iteration 159: Total Loss: 0.2729330062866211, Expl Loss: 0.5434626340866089, Output Loss: 0.02403366006910801\n",
      "Iteration 160: Total Loss: 0.2728564441204071, Expl Loss: 0.5433014631271362, Output Loss: 0.024114182218909264\n",
      "Iteration 161: Total Loss: 0.2727801203727722, Expl Loss: 0.5431497693061829, Output Loss: 0.024104613810777664\n",
      "Iteration 162: Total Loss: 0.27270403504371643, Expl Loss: 0.5430057048797607, Output Loss: 0.024023722857236862\n",
      "Iteration 163: Total Loss: 0.2726287543773651, Expl Loss: 0.5428677201271057, Output Loss: 0.02389766275882721\n",
      "Iteration 164: Total Loss: 0.27255332469940186, Expl Loss: 0.5427226424217224, Output Loss: 0.023840103298425674\n",
      "Iteration 165: Total Loss: 0.2724788188934326, Expl Loss: 0.5425717234611511, Output Loss: 0.02385900542140007\n",
      "Iteration 166: Total Loss: 0.2724045217037201, Expl Loss: 0.5424162149429321, Output Loss: 0.023928478360176086\n",
      "Iteration 167: Total Loss: 0.2723309099674225, Expl Loss: 0.542264997959137, Output Loss: 0.023968370631337166\n",
      "Iteration 168: Total Loss: 0.2722568213939667, Expl Loss: 0.5421149730682373, Output Loss: 0.023986702784895897\n",
      "Iteration 169: Total Loss: 0.27218306064605713, Expl Loss: 0.5419704914093018, Output Loss: 0.023956144228577614\n",
      "Iteration 170: Total Loss: 0.2721095085144043, Expl Loss: 0.5418330430984497, Output Loss: 0.023859618231654167\n",
      "Iteration 171: Total Loss: 0.2720364034175873, Expl Loss: 0.541699230670929, Output Loss: 0.023735856637358665\n",
      "Iteration 172: Total Loss: 0.27196383476257324, Expl Loss: 0.541565477848053, Output Loss: 0.023621872067451477\n",
      "Iteration 173: Total Loss: 0.271891713142395, Expl Loss: 0.5414299964904785, Output Loss: 0.023534031584858894\n",
      "Iteration 174: Total Loss: 0.2718198895454407, Expl Loss: 0.5412864089012146, Output Loss: 0.023533735424280167\n",
      "Iteration 175: Total Loss: 0.2717486023902893, Expl Loss: 0.541136622428894, Output Loss: 0.023605937138199806\n",
      "Iteration 176: Total Loss: 0.271677702665329, Expl Loss: 0.540988028049469, Output Loss: 0.023673923686146736\n",
      "Iteration 177: Total Loss: 0.2716068625450134, Expl Loss: 0.5408464074134827, Output Loss: 0.023672981187701225\n",
      "Iteration 178: Total Loss: 0.27153655886650085, Expl Loss: 0.5407111644744873, Output Loss: 0.023619359359145164\n",
      "Iteration 179: Total Loss: 0.27146610617637634, Expl Loss: 0.5405817031860352, Output Loss: 0.02350526675581932\n",
      "Iteration 180: Total Loss: 0.2713937759399414, Expl Loss: 0.5404502749443054, Output Loss: 0.02337266504764557\n",
      "Iteration 181: Total Loss: 0.271321564912796, Expl Loss: 0.5403170585632324, Output Loss: 0.02326084114611149\n",
      "Iteration 182: Total Loss: 0.2712489068508148, Expl Loss: 0.5401740670204163, Output Loss: 0.023237545043230057\n",
      "Iteration 183: Total Loss: 0.2711760699748993, Expl Loss: 0.5400239825248718, Output Loss: 0.023281747475266457\n",
      "Iteration 184: Total Loss: 0.27110323309898376, Expl Loss: 0.5398736596107483, Output Loss: 0.023328011855483055\n",
      "Iteration 185: Total Loss: 0.2710303068161011, Expl Loss: 0.5397265553474426, Output Loss: 0.023340579122304916\n",
      "Iteration 186: Total Loss: 0.2709573805332184, Expl Loss: 0.539584755897522, Output Loss: 0.023299846798181534\n",
      "Iteration 187: Total Loss: 0.27088454365730286, Expl Loss: 0.5394482612609863, Output Loss: 0.02320808544754982\n",
      "Iteration 188: Total Loss: 0.27081161737442017, Expl Loss: 0.5393118262290955, Output Loss: 0.02311434969305992\n",
      "Iteration 189: Total Loss: 0.2707389295101166, Expl Loss: 0.5391730666160583, Output Loss: 0.02304794266819954\n",
      "Iteration 190: Total Loss: 0.27066633105278015, Expl Loss: 0.5390294194221497, Output Loss: 0.023032307624816895\n",
      "Iteration 191: Total Loss: 0.27059420943260193, Expl Loss: 0.5388813018798828, Output Loss: 0.023071151226758957\n",
      "Iteration 192: Total Loss: 0.27052271366119385, Expl Loss: 0.5387332439422607, Output Loss: 0.023121695965528488\n",
      "Iteration 193: Total Loss: 0.27045151591300964, Expl Loss: 0.5385894179344177, Output Loss: 0.02313595451414585\n",
      "Iteration 194: Total Loss: 0.2703808844089508, Expl Loss: 0.5384518504142761, Output Loss: 0.023098979145288467\n",
      "Iteration 195: Total Loss: 0.2703111171722412, Expl Loss: 0.5383219718933105, Output Loss: 0.02300274930894375\n",
      "Iteration 196: Total Loss: 0.27024245262145996, Expl Loss: 0.5381966233253479, Output Loss: 0.022882718592882156\n",
      "Iteration 197: Total Loss: 0.27017438411712646, Expl Loss: 0.5380651950836182, Output Loss: 0.02283584326505661\n",
      "Iteration 198: Total Loss: 0.270107239484787, Expl Loss: 0.5379244089126587, Output Loss: 0.022900618612766266\n",
      "Iteration 199: Total Loss: 0.2700411081314087, Expl Loss: 0.5377807021141052, Output Loss: 0.0230148583650589\n",
      "Iteration 200: Total Loss: 0.2699762284755707, Expl Loss: 0.5376444458961487, Output Loss: 0.02307993732392788\n",
      "Iteration 201: Total Loss: 0.26991161704063416, Expl Loss: 0.537514328956604, Output Loss: 0.023089224472641945\n",
      "Iteration 202: Total Loss: 0.2698477804660797, Expl Loss: 0.5373913645744324, Output Loss: 0.02304193377494812\n",
      "Iteration 203: Total Loss: 0.26978451013565063, Expl Loss: 0.5372710227966309, Output Loss: 0.022980183362960815\n",
      "Iteration 204: Total Loss: 0.26972201466560364, Expl Loss: 0.53714519739151, Output Loss: 0.022988587617874146\n",
      "Iteration 205: Total Loss: 0.26966044306755066, Expl Loss: 0.5370198488235474, Output Loss: 0.023010307922959328\n",
      "Iteration 206: Total Loss: 0.2695996165275574, Expl Loss: 0.536893904209137, Output Loss: 0.023053091019392014\n",
      "Iteration 207: Total Loss: 0.26953938603401184, Expl Loss: 0.5367680191993713, Output Loss: 0.02310744859278202\n",
      "Iteration 208: Total Loss: 0.2694794535636902, Expl Loss: 0.5366390943527222, Output Loss: 0.023198258131742477\n",
      "Iteration 209: Total Loss: 0.26942020654678345, Expl Loss: 0.5365144610404968, Output Loss: 0.02325928956270218\n",
      "Iteration 210: Total Loss: 0.2693614065647125, Expl Loss: 0.536399245262146, Output Loss: 0.023235607892274857\n",
      "Iteration 211: Total Loss: 0.26930299401283264, Expl Loss: 0.536289393901825, Output Loss: 0.023165728896856308\n",
      "Iteration 212: Total Loss: 0.26924508810043335, Expl Loss: 0.5361770987510681, Output Loss: 0.023130768910050392\n",
      "Iteration 213: Total Loss: 0.2691877782344818, Expl Loss: 0.5360580682754517, Output Loss: 0.023175004869699478\n",
      "Iteration 214: Total Loss: 0.2691310942173004, Expl Loss: 0.535935640335083, Output Loss: 0.023265713825821877\n",
      "Iteration 215: Total Loss: 0.26907479763031006, Expl Loss: 0.5358113050460815, Output Loss: 0.023382896557450294\n",
      "Iteration 216: Total Loss: 0.26901906728744507, Expl Loss: 0.5356917381286621, Output Loss: 0.023464133962988853\n",
      "Iteration 217: Total Loss: 0.26896414160728455, Expl Loss: 0.5355839133262634, Output Loss: 0.023443469777703285\n",
      "Iteration 218: Total Loss: 0.26891034841537476, Expl Loss: 0.5354761481285095, Output Loss: 0.023445751518011093\n",
      "Iteration 219: Total Loss: 0.2688572108745575, Expl Loss: 0.5353714823722839, Output Loss: 0.023429125547409058\n",
      "Iteration 220: Total Loss: 0.26880452036857605, Expl Loss: 0.5352665185928345, Output Loss: 0.023425335064530373\n",
      "Iteration 221: Total Loss: 0.2687522768974304, Expl Loss: 0.5351595878601074, Output Loss: 0.023449435830116272\n",
      "Iteration 222: Total Loss: 0.268700510263443, Expl Loss: 0.5350515246391296, Output Loss: 0.023494768887758255\n",
      "Iteration 223: Total Loss: 0.26864930987358093, Expl Loss: 0.534944474697113, Output Loss: 0.023541707545518875\n",
      "Iteration 224: Total Loss: 0.2685988247394562, Expl Loss: 0.5348446369171143, Output Loss: 0.023529835045337677\n",
      "Iteration 225: Total Loss: 0.268548846244812, Expl Loss: 0.534747302532196, Output Loss: 0.023503903299570084\n",
      "Iteration 226: Total Loss: 0.26849934458732605, Expl Loss: 0.5346477031707764, Output Loss: 0.02350975200533867\n",
      "Iteration 227: Total Loss: 0.2684502601623535, Expl Loss: 0.5345453023910522, Output Loss: 0.023552393540740013\n",
      "Iteration 228: Total Loss: 0.268401563167572, Expl Loss: 0.534443736076355, Output Loss: 0.023594195023179054\n",
      "Iteration 229: Total Loss: 0.26835429668426514, Expl Loss: 0.5343506336212158, Output Loss: 0.02357964776456356\n",
      "Iteration 230: Total Loss: 0.26830777525901794, Expl Loss: 0.5342590808868408, Output Loss: 0.023564472794532776\n",
      "Iteration 231: Total Loss: 0.26826173067092896, Expl Loss: 0.5341697335243225, Output Loss: 0.02353733405470848\n",
      "Iteration 232: Total Loss: 0.2682160437107086, Expl Loss: 0.5340761542320251, Output Loss: 0.023559048771858215\n",
      "Iteration 233: Total Loss: 0.2681712806224823, Expl Loss: 0.5339791178703308, Output Loss: 0.02363433502614498\n",
      "Iteration 234: Total Loss: 0.2681267559528351, Expl Loss: 0.5338847637176514, Output Loss: 0.02368733659386635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 235: Total Loss: 0.26808276772499084, Expl Loss: 0.5337992906570435, Output Loss: 0.02366257831454277\n",
      "Iteration 236: Total Loss: 0.26803919672966003, Expl Loss: 0.533717155456543, Output Loss: 0.023612547665834427\n",
      "Iteration 237: Total Loss: 0.2679958939552307, Expl Loss: 0.5336328744888306, Output Loss: 0.023588964715600014\n",
      "Iteration 238: Total Loss: 0.26795274019241333, Expl Loss: 0.5335441827774048, Output Loss: 0.023612771183252335\n",
      "Iteration 239: Total Loss: 0.26790979504585266, Expl Loss: 0.5334527492523193, Output Loss: 0.023668643087148666\n",
      "Iteration 240: Total Loss: 0.26786744594573975, Expl Loss: 0.5333643555641174, Output Loss: 0.023705078288912773\n",
      "Iteration 241: Total Loss: 0.26782578229904175, Expl Loss: 0.5332842469215393, Output Loss: 0.023672906681895256\n",
      "Iteration 242: Total Loss: 0.2677845358848572, Expl Loss: 0.533204972743988, Output Loss: 0.023640945553779602\n",
      "Iteration 243: Total Loss: 0.2677434980869293, Expl Loss: 0.5331209301948547, Output Loss: 0.023660626262426376\n",
      "Iteration 244: Total Loss: 0.26770272850990295, Expl Loss: 0.5330327153205872, Output Loss: 0.023727407678961754\n",
      "Iteration 245: Total Loss: 0.26766225695610046, Expl Loss: 0.5329446792602539, Output Loss: 0.023798491805791855\n",
      "Iteration 246: Total Loss: 0.2676222622394562, Expl Loss: 0.5328637957572937, Output Loss: 0.023807432502508163\n",
      "Iteration 247: Total Loss: 0.26758256554603577, Expl Loss: 0.532788872718811, Output Loss: 0.023762602359056473\n",
      "Iteration 248: Total Loss: 0.2675432562828064, Expl Loss: 0.5327162742614746, Output Loss: 0.02370239421725273\n",
      "Iteration 249: Total Loss: 0.26750412583351135, Expl Loss: 0.5326389074325562, Output Loss: 0.02369372360408306\n",
      "Iteration 250: Total Loss: 0.26746508479118347, Expl Loss: 0.5325543284416199, Output Loss: 0.023758631199598312\n",
      "Iteration 251: Total Loss: 0.2674263119697571, Expl Loss: 0.5324677228927612, Output Loss: 0.023848820477724075\n",
      "Iteration 252: Total Loss: 0.2673878073692322, Expl Loss: 0.5323866009712219, Output Loss: 0.02389037236571312\n",
      "Iteration 253: Total Loss: 0.267349511384964, Expl Loss: 0.5323125720024109, Output Loss: 0.02386460267007351\n",
      "Iteration 254: Total Loss: 0.26731133460998535, Expl Loss: 0.5322404503822327, Output Loss: 0.023822400718927383\n",
      "Iteration 255: Total Loss: 0.2672734558582306, Expl Loss: 0.532162606716156, Output Loss: 0.023843172937631607\n",
      "Iteration 256: Total Loss: 0.2672356069087982, Expl Loss: 0.5320824384689331, Output Loss: 0.02388792857527733\n",
      "Iteration 257: Total Loss: 0.26719820499420166, Expl Loss: 0.5320062041282654, Output Loss: 0.02390184998512268\n",
      "Iteration 258: Total Loss: 0.26716113090515137, Expl Loss: 0.5319326519966125, Output Loss: 0.023896288126707077\n",
      "Iteration 259: Total Loss: 0.2671242654323578, Expl Loss: 0.5318595767021179, Output Loss: 0.023889292031526566\n",
      "Iteration 260: Total Loss: 0.26708781719207764, Expl Loss: 0.5317854881286621, Output Loss: 0.02390126883983612\n",
      "Iteration 261: Total Loss: 0.26705241203308105, Expl Loss: 0.5317137837409973, Output Loss: 0.0239101629704237\n",
      "Iteration 262: Total Loss: 0.2670181095600128, Expl Loss: 0.5316451191902161, Output Loss: 0.02391108125448227\n",
      "Iteration 263: Total Loss: 0.2669840157032013, Expl Loss: 0.5315737724304199, Output Loss: 0.02394232712686062\n",
      "Iteration 264: Total Loss: 0.26695042848587036, Expl Loss: 0.5314987897872925, Output Loss: 0.02402080036699772\n",
      "Iteration 265: Total Loss: 0.2669173777103424, Expl Loss: 0.5314282774925232, Output Loss: 0.02406482771039009\n",
      "Iteration 266: Total Loss: 0.2668844759464264, Expl Loss: 0.5313685536384583, Output Loss: 0.02400388941168785\n",
      "Iteration 267: Total Loss: 0.26685160398483276, Expl Loss: 0.531312882900238, Output Loss: 0.02390328049659729\n",
      "Iteration 268: Total Loss: 0.26681894063949585, Expl Loss: 0.5312545299530029, Output Loss: 0.023833686485886574\n",
      "Iteration 269: Total Loss: 0.26678648591041565, Expl Loss: 0.5311889052391052, Output Loss: 0.02384079620242119\n",
      "Iteration 270: Total Loss: 0.2667540907859802, Expl Loss: 0.5311164259910583, Output Loss: 0.02391747198998928\n",
      "Iteration 271: Total Loss: 0.26672178506851196, Expl Loss: 0.5310441851615906, Output Loss: 0.023994024842977524\n",
      "Iteration 272: Total Loss: 0.26668959856033325, Expl Loss: 0.5309792757034302, Output Loss: 0.023999255150556564\n",
      "Iteration 273: Total Loss: 0.2666575610637665, Expl Loss: 0.5309205055236816, Output Loss: 0.023945990949869156\n",
      "Iteration 274: Total Loss: 0.2666255235671997, Expl Loss: 0.5308629870414734, Output Loss: 0.023880600929260254\n",
      "Iteration 275: Total Loss: 0.26659372448921204, Expl Loss: 0.5308030843734741, Output Loss: 0.023843463510274887\n",
      "Iteration 276: Total Loss: 0.26656246185302734, Expl Loss: 0.530738890171051, Output Loss: 0.023860204964876175\n",
      "Iteration 277: Total Loss: 0.2665313482284546, Expl Loss: 0.5306723713874817, Output Loss: 0.02390347793698311\n",
      "Iteration 278: Total Loss: 0.2665002644062042, Expl Loss: 0.5306078195571899, Output Loss: 0.02392696775496006\n",
      "Iteration 279: Total Loss: 0.26646915078163147, Expl Loss: 0.5305480360984802, Output Loss: 0.023902449756860733\n",
      "Iteration 280: Total Loss: 0.2664380967617035, Expl Loss: 0.5304926037788391, Output Loss: 0.02383597381412983\n",
      "Iteration 281: Total Loss: 0.2664071023464203, Expl Loss: 0.530437171459198, Output Loss: 0.023770369589328766\n",
      "Iteration 282: Total Loss: 0.26637616753578186, Expl Loss: 0.5303753614425659, Output Loss: 0.023769602179527283\n",
      "Iteration 283: Total Loss: 0.26634538173675537, Expl Loss: 0.5303072929382324, Output Loss: 0.02383444830775261\n",
      "Iteration 284: Total Loss: 0.26631486415863037, Expl Loss: 0.5302475690841675, Output Loss: 0.023821547627449036\n",
      "Iteration 285: Total Loss: 0.26628392934799194, Expl Loss: 0.5301894545555115, Output Loss: 0.02378382720053196\n",
      "Iteration 286: Total Loss: 0.2662528455257416, Expl Loss: 0.5301297903060913, Output Loss: 0.023758986964821815\n",
      "Iteration 287: Total Loss: 0.2662215232849121, Expl Loss: 0.530067503452301, Output Loss: 0.023755457252264023\n",
      "Iteration 288: Total Loss: 0.2661900520324707, Expl Loss: 0.5300043821334839, Output Loss: 0.02375698648393154\n",
      "Iteration 289: Total Loss: 0.26615825295448303, Expl Loss: 0.5299419164657593, Output Loss: 0.023745959624648094\n",
      "Iteration 290: Total Loss: 0.26612621545791626, Expl Loss: 0.5298807621002197, Output Loss: 0.023716481402516365\n",
      "Iteration 291: Total Loss: 0.2660941779613495, Expl Loss: 0.5298231244087219, Output Loss: 0.023652594536542892\n",
      "Iteration 292: Total Loss: 0.26606249809265137, Expl Loss: 0.5297635793685913, Output Loss: 0.023614149540662766\n",
      "Iteration 293: Total Loss: 0.26603052020072937, Expl Loss: 0.5297013521194458, Output Loss: 0.02359689213335514\n",
      "Iteration 294: Total Loss: 0.2659984230995178, Expl Loss: 0.52963787317276, Output Loss: 0.023589450865983963\n",
      "Iteration 295: Total Loss: 0.2659659683704376, Expl Loss: 0.5295752882957458, Output Loss: 0.02356642484664917\n",
      "Iteration 296: Total Loss: 0.2659332752227783, Expl Loss: 0.5295177102088928, Output Loss: 0.023488659411668777\n",
      "Iteration 297: Total Loss: 0.26590028405189514, Expl Loss: 0.5294597744941711, Output Loss: 0.023408204317092896\n",
      "Iteration 298: Total Loss: 0.26586681604385376, Expl Loss: 0.5293974876403809, Output Loss: 0.023361647501587868\n",
      "Iteration 299: Total Loss: 0.2658332884311676, Expl Loss: 0.5293323993682861, Output Loss: 0.023341665044426918\n",
      "Iteration 300: Total Loss: 0.2657993733882904, Expl Loss: 0.5292688608169556, Output Loss: 0.02329861745238304\n",
      "Iteration 301: Total Loss: 0.26576530933380127, Expl Loss: 0.5292060971260071, Output Loss: 0.023244937881827354\n",
      "Iteration 302: Total Loss: 0.2657318413257599, Expl Loss: 0.5291518568992615, Output Loss: 0.023118460550904274\n",
      "Iteration 303: Total Loss: 0.26569893956184387, Expl Loss: 0.5290964245796204, Output Loss: 0.0230142530053854\n",
      "Iteration 304: Total Loss: 0.2656668722629547, Expl Loss: 0.5290420055389404, Output Loss: 0.02291729673743248\n",
      "Iteration 305: Total Loss: 0.26563507318496704, Expl Loss: 0.5289797782897949, Output Loss: 0.022903526201844215\n",
      "Iteration 306: Total Loss: 0.2656034529209137, Expl Loss: 0.5289108753204346, Output Loss: 0.02296019159257412\n",
      "Iteration 307: Total Loss: 0.26557183265686035, Expl Loss: 0.5288456678390503, Output Loss: 0.022980207577347755\n",
      "Iteration 308: Total Loss: 0.26554033160209656, Expl Loss: 0.5287901759147644, Output Loss: 0.022905156016349792\n",
      "Iteration 309: Total Loss: 0.26550906896591187, Expl Loss: 0.5287450551986694, Output Loss: 0.022730808705091476\n",
      "Iteration 310: Total Loss: 0.26547807455062866, Expl Loss: 0.5286948680877686, Output Loss: 0.02261285111308098\n",
      "Iteration 311: Total Loss: 0.26544713973999023, Expl Loss: 0.5286315083503723, Output Loss: 0.02262790873646736\n",
      "Iteration 312: Total Loss: 0.26541638374328613, Expl Loss: 0.5285601615905762, Output Loss: 0.022726189345121384\n",
      "Iteration 313: Total Loss: 0.26538634300231934, Expl Loss: 0.5284955501556396, Output Loss: 0.02277158573269844\n",
      "Iteration 314: Total Loss: 0.26535630226135254, Expl Loss: 0.5284386277198792, Output Loss: 0.022739946842193604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 315: Total Loss: 0.26532652974128723, Expl Loss: 0.5283861756324768, Output Loss: 0.022668888792395592\n",
      "Iteration 316: Total Loss: 0.2652970254421234, Expl Loss: 0.52833092212677, Output Loss: 0.022631287574768066\n",
      "Iteration 317: Total Loss: 0.26526767015457153, Expl Loss: 0.5282684564590454, Output Loss: 0.022668812423944473\n",
      "Iteration 318: Total Loss: 0.2652384638786316, Expl Loss: 0.5281992554664612, Output Loss: 0.022776704281568527\n",
      "Iteration 319: Total Loss: 0.26520973443984985, Expl Loss: 0.5281363725662231, Output Loss: 0.022830884903669357\n",
      "Iteration 320: Total Loss: 0.26518115401268005, Expl Loss: 0.5280753374099731, Output Loss: 0.022869711741805077\n",
      "Iteration 321: Total Loss: 0.26515278220176697, Expl Loss: 0.5280144214630127, Output Loss: 0.02291116490960121\n",
      "Iteration 322: Total Loss: 0.2651247978210449, Expl Loss: 0.5279480814933777, Output Loss: 0.023015234619379044\n",
      "Iteration 323: Total Loss: 0.2650972604751587, Expl Loss: 0.5278810262680054, Output Loss: 0.02313479408621788\n",
      "Iteration 324: Total Loss: 0.2650698721408844, Expl Loss: 0.5278178453445435, Output Loss: 0.023218709975481033\n",
      "Iteration 325: Total Loss: 0.2650425434112549, Expl Loss: 0.5277578830718994, Output Loss: 0.023271959275007248\n",
      "Iteration 326: Total Loss: 0.2650156319141388, Expl Loss: 0.527696430683136, Output Loss: 0.023348070681095123\n",
      "Iteration 327: Total Loss: 0.26498883962631226, Expl Loss: 0.5276339650154114, Output Loss: 0.02343718148767948\n",
      "Iteration 328: Total Loss: 0.26496243476867676, Expl Loss: 0.5275722742080688, Output Loss: 0.0235257800668478\n",
      "Iteration 329: Total Loss: 0.2649363875389099, Expl Loss: 0.527511477470398, Output Loss: 0.0236128531396389\n",
      "Iteration 330: Total Loss: 0.26491060853004456, Expl Loss: 0.5274523496627808, Output Loss: 0.023688873276114464\n",
      "Iteration 331: Total Loss: 0.26488494873046875, Expl Loss: 0.5273927450180054, Output Loss: 0.02377176098525524\n",
      "Iteration 332: Total Loss: 0.2648595869541168, Expl Loss: 0.5273351669311523, Output Loss: 0.023840036243200302\n",
      "Iteration 333: Total Loss: 0.2648349404335022, Expl Loss: 0.5272843241691589, Output Loss: 0.023855334147810936\n",
      "Iteration 334: Total Loss: 0.2648107707500458, Expl Loss: 0.5272244811058044, Output Loss: 0.02397068403661251\n",
      "Iteration 335: Total Loss: 0.2647872269153595, Expl Loss: 0.5271600484848022, Output Loss: 0.02414414845407009\n",
      "Iteration 336: Total Loss: 0.2647637128829956, Expl Loss: 0.5271078944206238, Output Loss: 0.02419532835483551\n",
      "Iteration 337: Total Loss: 0.2647407054901123, Expl Loss: 0.527070164680481, Output Loss: 0.024112224578857422\n",
      "Iteration 338: Total Loss: 0.26471802592277527, Expl Loss: 0.5270321369171143, Output Loss: 0.024039234966039658\n",
      "Iteration 339: Total Loss: 0.2646954655647278, Expl Loss: 0.5269817113876343, Output Loss: 0.024091966450214386\n",
      "Iteration 340: Total Loss: 0.2646732032299042, Expl Loss: 0.526925802230835, Output Loss: 0.024206319823861122\n",
      "Iteration 341: Total Loss: 0.26465120911598206, Expl Loss: 0.5268714427947998, Output Loss: 0.024309804663062096\n",
      "Iteration 342: Total Loss: 0.26462966203689575, Expl Loss: 0.5268222689628601, Output Loss: 0.024370742961764336\n",
      "Iteration 343: Total Loss: 0.26460811495780945, Expl Loss: 0.5267826914787292, Output Loss: 0.024335604161024094\n",
      "Iteration 344: Total Loss: 0.2645866870880127, Expl Loss: 0.5267468094825745, Output Loss: 0.024265620857477188\n",
      "Iteration 345: Total Loss: 0.26456546783447266, Expl Loss: 0.5267074108123779, Output Loss: 0.024235237389802933\n",
      "Iteration 346: Total Loss: 0.2645443081855774, Expl Loss: 0.5266585350036621, Output Loss: 0.02430090308189392\n",
      "Iteration 347: Total Loss: 0.2645232081413269, Expl Loss: 0.5266056656837463, Output Loss: 0.024407653138041496\n",
      "Iteration 348: Total Loss: 0.26450249552726746, Expl Loss: 0.5265569090843201, Output Loss: 0.02448064088821411\n",
      "Iteration 349: Total Loss: 0.26448193192481995, Expl Loss: 0.5265169143676758, Output Loss: 0.024469569325447083\n",
      "Iteration 350: Total Loss: 0.264461874961853, Expl Loss: 0.5264798402786255, Output Loss: 0.024438999593257904\n",
      "Iteration 351: Total Loss: 0.26444196701049805, Expl Loss: 0.526443600654602, Output Loss: 0.02440357767045498\n",
      "Iteration 352: Total Loss: 0.2644222378730774, Expl Loss: 0.5264032483100891, Output Loss: 0.0244121216237545\n",
      "Iteration 353: Total Loss: 0.2644028067588806, Expl Loss: 0.5263558030128479, Output Loss: 0.02449817582964897\n",
      "Iteration 354: Total Loss: 0.26438358426094055, Expl Loss: 0.5263133645057678, Output Loss: 0.02453801967203617\n",
      "Iteration 355: Total Loss: 0.2643645107746124, Expl Loss: 0.5262782573699951, Output Loss: 0.02450738474726677\n",
      "Iteration 356: Total Loss: 0.26434576511383057, Expl Loss: 0.5262431502342224, Output Loss: 0.024484094232320786\n",
      "Iteration 357: Total Loss: 0.2643273174762726, Expl Loss: 0.5262015461921692, Output Loss: 0.02453072927892208\n",
      "Iteration 358: Total Loss: 0.26430898904800415, Expl Loss: 0.5261596441268921, Output Loss: 0.024583186954259872\n",
      "Iteration 359: Total Loss: 0.26429077982902527, Expl Loss: 0.5261210799217224, Output Loss: 0.024604754522442818\n",
      "Iteration 360: Total Loss: 0.2642728388309479, Expl Loss: 0.5260874032974243, Output Loss: 0.02458258345723152\n",
      "Iteration 361: Total Loss: 0.26425492763519287, Expl Loss: 0.526053249835968, Output Loss: 0.024566177278757095\n",
      "Iteration 362: Total Loss: 0.2642371654510498, Expl Loss: 0.5260146260261536, Output Loss: 0.024596964940428734\n",
      "Iteration 363: Total Loss: 0.26421940326690674, Expl Loss: 0.5259731411933899, Output Loss: 0.024656448513269424\n",
      "Iteration 364: Total Loss: 0.2642018496990204, Expl Loss: 0.5259331464767456, Output Loss: 0.02470530942082405\n",
      "Iteration 365: Total Loss: 0.26418453454971313, Expl Loss: 0.5258992910385132, Output Loss: 0.024697493761777878\n",
      "Iteration 366: Total Loss: 0.2641677260398865, Expl Loss: 0.5258739590644836, Output Loss: 0.024614837020635605\n",
      "Iteration 367: Total Loss: 0.26415109634399414, Expl Loss: 0.5258419513702393, Output Loss: 0.02460259571671486\n",
      "Iteration 368: Total Loss: 0.2641344964504242, Expl Loss: 0.5258015394210815, Output Loss: 0.02467457577586174\n",
      "Iteration 369: Total Loss: 0.2641180455684662, Expl Loss: 0.5257580876350403, Output Loss: 0.024779921397566795\n",
      "Iteration 370: Total Loss: 0.26410162448883057, Expl Loss: 0.5257201790809631, Output Loss: 0.0248308926820755\n",
      "Iteration 371: Total Loss: 0.2640850841999054, Expl Loss: 0.5256901383399963, Output Loss: 0.024800151586532593\n",
      "Iteration 372: Total Loss: 0.26406845450401306, Expl Loss: 0.5256619453430176, Output Loss: 0.024749845266342163\n",
      "Iteration 373: Total Loss: 0.2640519440174103, Expl Loss: 0.5256291031837463, Output Loss: 0.024747828021645546\n",
      "Iteration 374: Total Loss: 0.2640356421470642, Expl Loss: 0.5255942940711975, Output Loss: 0.024769995361566544\n",
      "Iteration 375: Total Loss: 0.26401975750923157, Expl Loss: 0.5255548357963562, Output Loss: 0.024846529588103294\n",
      "Iteration 376: Total Loss: 0.2640041708946228, Expl Loss: 0.5255162119865417, Output Loss: 0.024921396747231483\n",
      "Iteration 377: Total Loss: 0.26398882269859314, Expl Loss: 0.5254863500595093, Output Loss: 0.024912936612963676\n",
      "Iteration 378: Total Loss: 0.26397350430488586, Expl Loss: 0.5254597067832947, Output Loss: 0.024873027577996254\n",
      "Iteration 379: Total Loss: 0.2639581561088562, Expl Loss: 0.5254299640655518, Output Loss: 0.024863507598638535\n",
      "Iteration 380: Total Loss: 0.2639424502849579, Expl Loss: 0.5253933668136597, Output Loss: 0.02491527423262596\n",
      "Iteration 381: Total Loss: 0.26392677426338196, Expl Loss: 0.5253542065620422, Output Loss: 0.024993184953927994\n",
      "Iteration 382: Total Loss: 0.2639109790325165, Expl Loss: 0.525320291519165, Output Loss: 0.02501649782061577\n",
      "Iteration 383: Total Loss: 0.2638951539993286, Expl Loss: 0.5252925157546997, Output Loss: 0.024978095665574074\n",
      "Iteration 384: Total Loss: 0.26387929916381836, Expl Loss: 0.5252655148506165, Output Loss: 0.024930881336331367\n",
      "Iteration 385: Total Loss: 0.26386338472366333, Expl Loss: 0.5252335071563721, Output Loss: 0.024932514876127243\n",
      "Iteration 386: Total Loss: 0.26384755969047546, Expl Loss: 0.5251967906951904, Output Loss: 0.02498319372534752\n",
      "Iteration 387: Total Loss: 0.2638316750526428, Expl Loss: 0.5251592397689819, Output Loss: 0.025040948763489723\n",
      "Iteration 388: Total Loss: 0.26381614804267883, Expl Loss: 0.5251242518424988, Output Loss: 0.025080686435103416\n",
      "Iteration 389: Total Loss: 0.26380065083503723, Expl Loss: 0.5250970721244812, Output Loss: 0.02504255808889866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 390: Total Loss: 0.26378533244132996, Expl Loss: 0.5250771045684814, Output Loss: 0.024935638532042503\n",
      "Iteration 391: Total Loss: 0.2637702524662018, Expl Loss: 0.5250528454780579, Output Loss: 0.024876315146684647\n",
      "Iteration 392: Total Loss: 0.2637553811073303, Expl Loss: 0.5250187516212463, Output Loss: 0.024919863790273666\n",
      "Iteration 393: Total Loss: 0.263740599155426, Expl Loss: 0.5249796509742737, Output Loss: 0.025015419349074364\n",
      "Iteration 394: Total Loss: 0.2637259364128113, Expl Loss: 0.5249457359313965, Output Loss: 0.025061583146452904\n",
      "Iteration 395: Total Loss: 0.2637113928794861, Expl Loss: 0.5249245762825012, Output Loss: 0.02498188428580761\n",
      "Iteration 396: Total Loss: 0.2636969983577728, Expl Loss: 0.5249053835868835, Output Loss: 0.024886270985007286\n",
      "Iteration 397: Total Loss: 0.26368266344070435, Expl Loss: 0.5248781442642212, Output Loss: 0.02487189695239067\n",
      "Iteration 398: Total Loss: 0.2636684477329254, Expl Loss: 0.5248451828956604, Output Loss: 0.024917062371969223\n",
      "Iteration 399: Total Loss: 0.263654500246048, Expl Loss: 0.5248103737831116, Output Loss: 0.024986188858747482\n",
      "Iteration 400: Total Loss: 0.26364079117774963, Expl Loss: 0.524777889251709, Output Loss: 0.025036906823515892\n",
      "Iteration 401: Total Loss: 0.26362714171409607, Expl Loss: 0.5247550010681152, Output Loss: 0.024993116036057472\n",
      "Iteration 402: Total Loss: 0.26361358165740967, Expl Loss: 0.5247349739074707, Output Loss: 0.0249218437820673\n",
      "Iteration 403: Total Loss: 0.26360011100769043, Expl Loss: 0.5247103571891785, Output Loss: 0.024898752570152283\n",
      "Iteration 404: Total Loss: 0.2635866701602936, Expl Loss: 0.524678111076355, Output Loss: 0.024952420964837074\n",
      "Iteration 405: Total Loss: 0.26357322931289673, Expl Loss: 0.5246436595916748, Output Loss: 0.025027787312865257\n",
      "Iteration 406: Total Loss: 0.26356008648872375, Expl Loss: 0.5246121287345886, Output Loss: 0.025080382823944092\n",
      "Iteration 407: Total Loss: 0.2635470926761627, Expl Loss: 0.5245875120162964, Output Loss: 0.025066977366805077\n",
      "Iteration 408: Total Loss: 0.2635346055030823, Expl Loss: 0.5245662331581116, Output Loss: 0.02502959966659546\n",
      "Iteration 409: Total Loss: 0.2635222375392914, Expl Loss: 0.5245464444160461, Output Loss: 0.024980396032333374\n",
      "Iteration 410: Total Loss: 0.26350998878479004, Expl Loss: 0.524523913860321, Output Loss: 0.02496071718633175\n",
      "Iteration 411: Total Loss: 0.2634977400302887, Expl Loss: 0.5244967937469482, Output Loss: 0.02498701587319374\n",
      "Iteration 412: Total Loss: 0.2634859085083008, Expl Loss: 0.5244698524475098, Output Loss: 0.02501944825053215\n",
      "Iteration 413: Total Loss: 0.2634742558002472, Expl Loss: 0.5244454145431519, Output Loss: 0.02503098174929619\n",
      "Iteration 414: Total Loss: 0.263462632894516, Expl Loss: 0.5244235396385193, Output Loss: 0.02501731552183628\n",
      "Iteration 415: Total Loss: 0.2634510397911072, Expl Loss: 0.5244022011756897, Output Loss: 0.024998817592859268\n",
      "Iteration 416: Total Loss: 0.26343950629234314, Expl Loss: 0.5243792533874512, Output Loss: 0.02499733492732048\n",
      "Iteration 417: Total Loss: 0.26342806220054626, Expl Loss: 0.5243537425994873, Output Loss: 0.025023633614182472\n",
      "Iteration 418: Total Loss: 0.2634166181087494, Expl Loss: 0.5243282914161682, Output Loss: 0.02504933439195156\n",
      "Iteration 419: Total Loss: 0.2634051740169525, Expl Loss: 0.5243048667907715, Output Loss: 0.025054607540369034\n",
      "Iteration 420: Total Loss: 0.26339372992515564, Expl Loss: 0.524283230304718, Output Loss: 0.025042450055480003\n",
      "Iteration 421: Total Loss: 0.26338231563568115, Expl Loss: 0.5242611765861511, Output Loss: 0.02503451704978943\n",
      "Iteration 422: Total Loss: 0.26337093114852905, Expl Loss: 0.5242384076118469, Output Loss: 0.025034600868821144\n",
      "Iteration 423: Total Loss: 0.2633596360683441, Expl Loss: 0.5242128372192383, Output Loss: 0.025064483284950256\n",
      "Iteration 424: Total Loss: 0.26334860920906067, Expl Loss: 0.5241808295249939, Output Loss: 0.025163620710372925\n",
      "Iteration 425: Total Loss: 0.2633376717567444, Expl Loss: 0.5241546034812927, Output Loss: 0.025207530707120895\n",
      "Iteration 426: Total Loss: 0.2633267641067505, Expl Loss: 0.5241398215293884, Output Loss: 0.025136996060609818\n",
      "Iteration 427: Total Loss: 0.2633158564567566, Expl Loss: 0.5241274237632751, Output Loss: 0.025042984634637833\n",
      "Iteration 428: Total Loss: 0.2633049786090851, Expl Loss: 0.5241068601608276, Output Loss: 0.02503097988665104\n",
      "Iteration 429: Total Loss: 0.26329416036605835, Expl Loss: 0.5240781903266907, Output Loss: 0.025101076811552048\n",
      "Iteration 430: Total Loss: 0.26328355073928833, Expl Loss: 0.5240490436553955, Output Loss: 0.02518080174922943\n",
      "Iteration 431: Total Loss: 0.26327311992645264, Expl Loss: 0.5240259170532227, Output Loss: 0.025203142315149307\n",
      "Iteration 432: Total Loss: 0.26326262950897217, Expl Loss: 0.5240074992179871, Output Loss: 0.0251776035875082\n",
      "Iteration 433: Total Loss: 0.2632521688938141, Expl Loss: 0.5239900350570679, Output Loss: 0.02514316514134407\n",
      "Iteration 434: Total Loss: 0.2632417380809784, Expl Loss: 0.5239694714546204, Output Loss: 0.025140006095170975\n",
      "Iteration 435: Total Loss: 0.2632313072681427, Expl Loss: 0.523942768573761, Output Loss: 0.025198232382535934\n",
      "Iteration 436: Total Loss: 0.2632210850715637, Expl Loss: 0.5239130258560181, Output Loss: 0.025291722267866135\n",
      "Iteration 437: Total Loss: 0.2632109522819519, Expl Loss: 0.5238902568817139, Output Loss: 0.02531631663441658\n",
      "Iteration 438: Total Loss: 0.2632007598876953, Expl Loss: 0.5238757133483887, Output Loss: 0.025258218869566917\n",
      "Iteration 439: Total Loss: 0.26319068670272827, Expl Loss: 0.5238612294197083, Output Loss: 0.025201648473739624\n",
      "Iteration 440: Total Loss: 0.26318052411079407, Expl Loss: 0.5238397717475891, Output Loss: 0.02521286904811859\n",
      "Iteration 441: Total Loss: 0.2631704807281494, Expl Loss: 0.5238128304481506, Output Loss: 0.025281066074967384\n",
      "Iteration 442: Total Loss: 0.26316049695014954, Expl Loss: 0.5237860083580017, Output Loss: 0.02534964680671692\n",
      "Iteration 443: Total Loss: 0.26315054297447205, Expl Loss: 0.5237652659416199, Output Loss: 0.025358015671372414\n",
      "Iteration 444: Total Loss: 0.2631404995918274, Expl Loss: 0.5237495303153992, Output Loss: 0.025314439088106155\n",
      "Iteration 445: Total Loss: 0.2631304860115051, Expl Loss: 0.5237336158752441, Output Loss: 0.025273621082305908\n",
      "Iteration 446: Total Loss: 0.26312053203582764, Expl Loss: 0.5237123370170593, Output Loss: 0.02528713084757328\n",
      "Iteration 447: Total Loss: 0.26311054825782776, Expl Loss: 0.5236859917640686, Output Loss: 0.025351176038384438\n",
      "Iteration 448: Total Loss: 0.26310068368911743, Expl Loss: 0.5236609578132629, Output Loss: 0.02540392242372036\n",
      "Iteration 449: Total Loss: 0.26309072971343994, Expl Loss: 0.5236415863037109, Output Loss: 0.02539871260523796\n",
      "Iteration 450: Total Loss: 0.2630808353424072, Expl Loss: 0.5236262083053589, Output Loss: 0.025354677811264992\n",
      "Iteration 451: Total Loss: 0.2630709409713745, Expl Loss: 0.5236091613769531, Output Loss: 0.025327404960989952\n",
      "Iteration 452: Total Loss: 0.2630611062049866, Expl Loss: 0.523586630821228, Output Loss: 0.02535560354590416\n",
      "Iteration 453: Total Loss: 0.26305121183395386, Expl Loss: 0.5235612988471985, Output Loss: 0.025411292910575867\n",
      "Iteration 454: Total Loss: 0.26304149627685547, Expl Loss: 0.5235368609428406, Output Loss: 0.02546113356947899\n",
      "Iteration 455: Total Loss: 0.26303181052207947, Expl Loss: 0.5235161185264587, Output Loss: 0.025474876165390015\n",
      "Iteration 456: Total Loss: 0.26302221417427063, Expl Loss: 0.5235002636909485, Output Loss: 0.025441888719797134\n",
      "Iteration 457: Total Loss: 0.26301270723342896, Expl Loss: 0.5234894752502441, Output Loss: 0.02535959519445896\n",
      "Iteration 458: Total Loss: 0.26300328969955444, Expl Loss: 0.5234732031822205, Output Loss: 0.025334030389785767\n",
      "Iteration 459: Total Loss: 0.26299384236335754, Expl Loss: 0.5234480500221252, Output Loss: 0.02539629302918911\n",
      "Iteration 460: Total Loss: 0.26298439502716064, Expl Loss: 0.5234217643737793, Output Loss: 0.02547016181051731\n",
      "Iteration 461: Total Loss: 0.2629750370979309, Expl Loss: 0.5234031677246094, Output Loss: 0.02546900138258934\n",
      "Iteration 462: Total Loss: 0.2629658579826355, Expl Loss: 0.5233909487724304, Output Loss: 0.025407541543245316\n",
      "Iteration 463: Total Loss: 0.26295676827430725, Expl Loss: 0.5233767628669739, Output Loss: 0.025367707014083862\n",
      "Iteration 464: Total Loss: 0.26294761896133423, Expl Loss: 0.5233548879623413, Output Loss: 0.02540341578423977\n",
      "Iteration 465: Total Loss: 0.2629384994506836, Expl Loss: 0.5233290791511536, Output Loss: 0.02547937072813511\n",
      "Iteration 466: Total Loss: 0.26292961835861206, Expl Loss: 0.5233045816421509, Output Loss: 0.025546401739120483\n",
      "Iteration 467: Total Loss: 0.2629207968711853, Expl Loss: 0.5232879519462585, Output Loss: 0.025536220520734787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 468: Total Loss: 0.26291224360466003, Expl Loss: 0.5232833027839661, Output Loss: 0.02541176974773407\n",
      "Iteration 469: Total Loss: 0.26290369033813477, Expl Loss: 0.5232720375061035, Output Loss: 0.02535361610352993\n",
      "Iteration 470: Total Loss: 0.2628951370716095, Expl Loss: 0.5232487320899963, Output Loss: 0.025415483862161636\n",
      "Iteration 471: Total Loss: 0.262886643409729, Expl Loss: 0.5232219099998474, Output Loss: 0.025514036417007446\n",
      "Iteration 472: Total Loss: 0.26287806034088135, Expl Loss: 0.523202121257782, Output Loss: 0.025539696216583252\n",
      "Iteration 473: Total Loss: 0.2628694474697113, Expl Loss: 0.523192822933197, Output Loss: 0.02546043135225773\n",
      "Iteration 474: Total Loss: 0.26286083459854126, Expl Loss: 0.5231824517250061, Output Loss: 0.025391941890120506\n",
      "Iteration 475: Total Loss: 0.26285216212272644, Expl Loss: 0.523163378238678, Output Loss: 0.025409528985619545\n",
      "Iteration 476: Total Loss: 0.2628434896469116, Expl Loss: 0.5231379270553589, Output Loss: 0.02549036778509617\n",
      "Iteration 477: Total Loss: 0.262834757566452, Expl Loss: 0.5231143236160278, Output Loss: 0.02555197849869728\n",
      "Iteration 478: Total Loss: 0.2628260850906372, Expl Loss: 0.523098349571228, Output Loss: 0.02553820237517357\n",
      "Iteration 479: Total Loss: 0.2628174126148224, Expl Loss: 0.5230872631072998, Output Loss: 0.02547590434551239\n",
      "Iteration 480: Total Loss: 0.2628089189529419, Expl Loss: 0.5230717658996582, Output Loss: 0.025460850447416306\n",
      "Iteration 481: Total Loss: 0.26280057430267334, Expl Loss: 0.5230522155761719, Output Loss: 0.025489067658782005\n",
      "Iteration 482: Total Loss: 0.2627921402454376, Expl Loss: 0.5230312943458557, Output Loss: 0.025530045852065086\n",
      "Iteration 483: Total Loss: 0.26278379559516907, Expl Loss: 0.5230141282081604, Output Loss: 0.02553452178835869\n",
      "Iteration 484: Total Loss: 0.2627755403518677, Expl Loss: 0.5230028033256531, Output Loss: 0.025482643395662308\n",
      "Iteration 485: Total Loss: 0.2627674341201782, Expl Loss: 0.5229921340942383, Output Loss: 0.025427211076021194\n",
      "Iteration 486: Total Loss: 0.262759268283844, Expl Loss: 0.5229767560958862, Output Loss: 0.025417858734726906\n",
      "Iteration 487: Total Loss: 0.2627512812614441, Expl Loss: 0.5229619741439819, Output Loss: 0.0254060085862875\n",
      "Iteration 488: Total Loss: 0.26274344325065613, Expl Loss: 0.5229405164718628, Output Loss: 0.025463754311203957\n",
      "Iteration 489: Total Loss: 0.26273566484451294, Expl Loss: 0.5229237079620361, Output Loss: 0.025476351380348206\n",
      "Iteration 490: Total Loss: 0.26272785663604736, Expl Loss: 0.522911548614502, Output Loss: 0.025441903620958328\n",
      "Iteration 491: Total Loss: 0.2627200782299042, Expl Loss: 0.5228999257087708, Output Loss: 0.02540246769785881\n",
      "Iteration 492: Total Loss: 0.2627123296260834, Expl Loss: 0.5228866338729858, Output Loss: 0.025380218401551247\n",
      "Iteration 493: Total Loss: 0.2627045810222626, Expl Loss: 0.5228717923164368, Output Loss: 0.0253734290599823\n",
      "Iteration 494: Total Loss: 0.2626968026161194, Expl Loss: 0.5228548049926758, Output Loss: 0.025388076901435852\n",
      "Iteration 495: Total Loss: 0.26268908381462097, Expl Loss: 0.5228386521339417, Output Loss: 0.025395000353455544\n",
      "Iteration 496: Total Loss: 0.2626812756061554, Expl Loss: 0.5228251814842224, Output Loss: 0.02537384070456028\n",
      "Iteration 497: Total Loss: 0.2626734673976898, Expl Loss: 0.5228133797645569, Output Loss: 0.025335758924484253\n",
      "Iteration 498: Total Loss: 0.26266565918922424, Expl Loss: 0.5228005647659302, Output Loss: 0.025307541713118553\n",
      "Iteration 499: Total Loss: 0.26265785098075867, Expl Loss: 0.5227852463722229, Output Loss: 0.02530459128320217\n"
     ]
    }
   ],
   "source": [
    "model.inputfooling_ON()\n",
    "\n",
    "x = np.asarray(mnist_testset[INDEX][0]).reshape(1,28,28)/255.0\n",
    "x = torch.Tensor(x)\n",
    "\n",
    "y, cls = model.classify(torch.Tensor(x[None, :]))\n",
    "target = torch.unsqueeze(cls, 0)\n",
    "\n",
    "noise = torch.rand(x.shape)*0.1\n",
    "x_adv = torch.Tensor(x + noise).clone().detach()\n",
    "\n",
    "\n",
    "\n",
    "# produce expls\n",
    "x.requires_grad = True\n",
    "org_expl = black_magic(model, x, cls)\n",
    "org_expl = org_expl.detach().cpu()\n",
    "x = x.detach()\n",
    "\n",
    "#targex *= 10 * torch.max(org_expl)\n",
    "target_expl = torch.Tensor(targex)\n",
    "target_expl = target_expl.detach()\n",
    "\n",
    "x_adv = torch.Tensor(x_adv[None, :])\n",
    "x_adv.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([x_adv], lr=0.001)\n",
    "\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_adv, cls = model.classify(x_adv)\n",
    "    grad_adv = torch.abs(black_magic(model, x_adv, cls))\n",
    "    \n",
    "    loss_expl = F.mse_loss(grad_adv, target_expl)\n",
    "    loss_output = F.mse_loss(y_adv, y.detach())\n",
    "    total_loss = 0.5 * loss_expl + 0.05 * loss_output\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Iteration {}: Total Loss: {}, Expl Loss: {}, Output Loss: {}\".format(i, total_loss.item(), loss_expl.item(), loss_output.item()))         \n",
    "    \n",
    "    x_adv.data = torch.clamp(x_adv.data, x - 0.2, x + 0.2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982bbc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.imshow(np.asarray(x.detach().numpy()).reshape(28,28))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.asarray(x_adv.detach().numpy()).reshape(28,28))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e7c326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.asarray(org_expl.detach().numpy()).reshape(28,28), cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.asarray(target_expl.detach().numpy()).reshape(28,28), cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.asarray(grad_adv.detach().numpy()).reshape(28,28), cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f06bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183, 206, 207, 208, 209, 210, 211, 453, 454, 455, 456, 457, 458, 460]\n",
      "[121, 122, 132, 150, 152, 184, 189, 191, 192, 193, 215, 274, 275, 344]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t = 1 - target_expl.detach().numpy().flatten()\n",
    "g = grad_adv.detach().numpy().flatten()\n",
    "\n",
    "tinds = list(np.argpartition(t, -14)[-14:])\n",
    "ginds = list(np.argpartition(g, -14)[-14:])\n",
    "print(sorted(tinds))\n",
    "print(sorted(ginds))\n",
    "print(set(tinds).isdisjoint(ginds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b80bc5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 10\n",
      "attack failure\n"
     ]
    }
   ],
   "source": [
    "adv_result = grad_adv.detach().numpy()\n",
    "inds = np.argmax(adv_result)\n",
    "max_i = int(inds/28); max_j = inds%28\n",
    "print(max_i, max_j)\n",
    "if(max_i > 8-3 and max_i < 8+3 and max_j > 8-3 and max_j < 8+3):\n",
    "    print(\"attack successful\")\n",
    "else:\n",
    "    print(\"attack failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65b34918",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2018558362.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/wk/dw_2kpzx6yggmhdyw5k5gj200000gn/T/ipykernel_55746/2018558362.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sadf asfd=  asdf\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sadf asfd=  asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from tqdm import trange \n",
    "\n",
    "def run_mnist_attack(model, x, target = 1, epsilon=0.2, iterations=250, lr=0.025):\n",
    "    model.inputfooling_ON()\n",
    "    y, cls = model.classify(torch.Tensor(x[None, :]))\n",
    "    noise = torch.rand(x.shape)*0.1\n",
    "    x_adv = torch.Tensor(x + noise).clone().detach()\n",
    "    \n",
    "    def get_gradient(model, x, desired_index):\n",
    "        # calculate the integrand in one batch\n",
    "        # we use DataParallel mode of model to fit the batch in memory of (multiple) gpu(s)\n",
    "        num_summands = 30\n",
    "        prefactors = x.new_tensor([k / num_summands for k in range(1, num_summands + 1)])\n",
    "        parallel_model = torch.nn.DataParallel(model)\n",
    "        y = parallel_model(prefactors.view(num_summands, 1, 1, 1) * x)\n",
    "\n",
    "        # we sum the result and then take the derivative (instead of summing derivatives as in most implementations),\n",
    "        # (d/dx) (n*y_1(1/n*x) + n/2*y_1(2/n*x) .... + y_n(x) ) = y_1'+....y'_n\n",
    "        y = torch.nn.functional.softmax(y, 1)[:, int(desired_index)]\n",
    "        y = (1 / num_summands) * torch.sum(y / prefactors, dim=0)\n",
    "        heatmap = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "        return heatmap\n",
    "\n",
    "    \n",
    "\n",
    "    # produce expls\n",
    "    x.requires_grad = True\n",
    "    org_expl = get_gradient(model, x, cls)\n",
    "    org_expl = org_expl.detach().cpu()\n",
    "    x = x.detach()\n",
    "    \n",
    "    # Generate target\n",
    "    target_locations = [[6,6], [14, 6], [21,6], [6, 21], [14, 21], [21,21]]\n",
    "    targ_i, targ_j = target_locations[target]\n",
    "    targex = np.asarray(x.detach().numpy()).reshape(28,28) * 0.0\n",
    "    targex[targ_i, targ_j] = 10\n",
    "    sigma = 3.0\n",
    "    targex = skimage.filters.gaussian(\n",
    "        targex, sigma=(sigma, sigma), truncate=3.5, multichannel=True)\n",
    "    targex = (targex-np.min(targex))/(np.max(targex)-np.min(targex))\n",
    "    targex *= 10 * np.max(org_expl.detach().numpy())\n",
    "    target_expl = torch.Tensor(targex)\n",
    "    target_expl = target_expl.detach()\n",
    "    \n",
    "    \n",
    "    # Generate adversarial attack\n",
    "    x_adv = torch.Tensor(x_adv[None, :])\n",
    "    x_adv.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam([x_adv], lr=lr)\n",
    "    value = 10\n",
    "    t = trange(iterations, desc=\"Loss: %s\"%(value))\n",
    "    for i in t:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_adv, cls = model.classify(x_adv)\n",
    "        grad_adv = get_gradient(model, x_adv, cls)\n",
    "\n",
    "        loss_expl = F.mse_loss(grad_adv, target_expl)\n",
    "        loss_output = F.mse_loss(y_adv, y.detach())\n",
    "        total_loss = 0.5 * loss_expl + 0.05 * loss_output\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"Iteration {}: Total Loss: {}, Expl Loss: {}, Output Loss: {}\".format(i, total_loss.item(), loss_expl.item(), loss_output.item()))         \n",
    "        x_adv.data = torch.clamp(x_adv.data, x - epsilon, x + epsilon)\n",
    "        \n",
    "        value = total_loss.item()\n",
    "        t.set_description(\"Loss: %s\"%(value))\n",
    "        t.refresh() # to show immediately the update\n",
    "    \n",
    "    success = False\n",
    "    adv_result = grad_adv.detach().numpy()\n",
    "    inds = np.argmax(adv_result)\n",
    "    max_i = int(inds/28); max_j = inds%28\n",
    "    if(max_i > targ_i-3 and max_i < targ_i+3 and max_j > targ_j-3 and max_j < targ_j+3):\n",
    "        success = True\n",
    "        \n",
    "    return success, x_adv, grad_adv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce690fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the non robust indexes and run a stronger attack on them \n",
    "ROBUST_COUNT = 0\n",
    "INDEX = 0\n",
    "for INDEX in range(100):\n",
    "    x = np.asarray(mnist_testset[INDEX][0]).reshape(1,28,28)/255.0\n",
    "    x = torch.Tensor(x)\n",
    "    success = False\n",
    "    for targ in range(6):\n",
    "        success, x_adv, grad_adv = run_mnist_attack(model, x, target = targ, epsilon=0.2, iterations=10)\n",
    "        if(success == True):\n",
    "            break\n",
    "    if(success == True):\n",
    "        print(\"vulnerable\")\n",
    "        continue\n",
    "    else:\n",
    "        ROBUST_COUNT += 1\n",
    "        print(\"Robust\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(ROBUST_COUNT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365abfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.asarray(x_adv.detach().numpy()).reshape(28,28), cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.asarray(grad_adv.detach().numpy()).reshape(28,28), cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
